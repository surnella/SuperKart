{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "k-xVqOB6QTm6"
   },
   "source": [
    "# **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "AJDbTeirQasg"
   },
   "source": [
    "## **Business Context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "94bMSnpWQmuA"
   },
   "source": [
    "A sales forecast is a prediction of future sales revenue based on historical data, industry trends, and the status of the current sales pipeline. Businesses use the sales forecast to estimate weekly, monthly, quarterly, and annual sales totals. A company needs to make an accurate sales forecast as it adds value across an organization and helps the different verticals to chalk out their future course of action.\n",
    "\n",
    "Forecasting helps an organization plan its sales operations by region and provides valuable insights to the supply chain team regarding the procurement of goods and materials. An accurate sales forecast process has many benefits which include improved decision-making about the future and reduction of sales pipeline and forecast risks. Moreover, it helps to reduce the time spent in planning territory coverage and establish benchmarks that can be used to assess trends in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "Aasy7LC_Qpq5"
   },
   "source": [
    "## **Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "khshBslaQtX9"
   },
   "source": [
    "SuperKart is a retail chain operating supermarkets and food marts across various tier cities, offering a wide range of products. To optimize its inventory management and make informed decisions around regional sales strategies, SuperKart wants to accurately forecast the sales revenue of its outlets for the upcoming quarter.\n",
    "\n",
    "To operationalize these insights at scale, the company has partnered with a data science firm—not just to build a predictive model based on historical sales data, but to develop and deploy a robust forecasting solution that can be integrated into SuperKart’s decision-making systems and used across its network of stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "v-HxlIhTQ0-E"
   },
   "source": [
    "## **Data Description**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "c0670116"
   },
   "source": [
    "The data contains the different attributes of the various products and stores.The detailed data dictionary is given below.\n",
    "\n",
    "- **Product_Id** - unique identifier of each product, each identifier having two letters at the beginning followed by a number.\n",
    "- **Product_Weight** - weight of each product\n",
    "- **Product_Sugar_Content** - sugar content of each product like low sugar, regular and no sugar\n",
    "- **Product_Allocated_Area** - ratio of the allocated display area of each product to the total display area of all the products in a store\n",
    "- **Product_Type** - broad category for each product like meat, snack foods, hard drinks, dairy, canned, soft drinks, health and hygiene, baking goods, bread, breakfast, frozen foods, fruits and vegetables, household, seafood, starchy foods, others\n",
    "- **Product_MRP** - maximum retail price of each product\n",
    "- **Store_Id** - unique identifier of each store\n",
    "- **Store_Establishment_Year** - year in which the store was established\n",
    "- **Store_Size** - size of the store depending on sq. feet like high, medium and low\n",
    "- **Store_Location_City_Type** - type of city in which the store is located like Tier 1, Tier 2 and Tier 3. Tier 1 consists of cities where the standard of living is comparatively higher than its Tier 2 and Tier 3 counterparts.\n",
    "- **Store_Type** - type of store depending on the products that are being sold there like Departmental Store, Supermarket Type 1, Supermarket Type 2 and Food Mart\n",
    "- **Product_Store_Sales_Total** - total revenue generated by the sale of that particular product in that particular store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3I-tX_4JWTa",
    "outputId": "16d9034a-1a24-45cc-bdd5-06ebc13def1d"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "60VhOlydQ-PG"
   },
   "source": [
    "# **Installing and Importing the necessary libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "d65d31d2"
   },
   "source": [
    "## **Install required packages - One time activity**\n",
    "- Commented this line as some pacakges are not compatible with the current environment. However joblib and hugging face are installed so the project is working.\n",
    "- Using VS Code with Jupyter extensions for running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "id": "yisDjKOB6TqF"
   },
   "outputs": [],
   "source": [
    "#Installing the libraries with the specified versions\n",
    "# !pip install numpy==2.0.2 pandas==2.2.2 scikit-learn==1.6.1 matplotlib==3.10.0 seaborn==0.13.2 joblib==1.4.2 xgboost==2.1.4 requests==2.32.3 huggingface_hub==0.30.1 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "m-wZg1XZ6bLa"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "- After running the above cell, kindly restart the notebook kernel (for Jupyter Notebook) or runtime (for Google Colab) and run all cells sequentially from the next cell.\n",
    "\n",
    "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "d7e7b718"
   },
   "source": [
    "## **Import the packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "c0022e4d"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Libraries to help with reading and manipulating data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Imports RobustScaler for data preprocessing\n",
    "from sklearn.preprocessing import RobustScaler, FunctionTransformer, OrdinalEncoder, MinMaxScaler\n",
    "\n",
    "# Libaries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# Removes the limit for the number of displayed columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Sets the limit for the number of displayed rows\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "\n",
    "#Import ensemble classifiers to be used for modelling.\n",
    "from sklearn.ensemble import (\n",
    "    BaggingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Libraries to get different metric scores for regression\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# To create the pipelines for data transformation of Test, Train and Validation datasets\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "\n",
    "# To tune different models and standardize\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# To serialize the model\n",
    "import joblib\n",
    "\n",
    "# os related functionalities\n",
    "import os\n",
    "\n",
    "# API request\n",
    "import requests\n",
    "import sklearn\n",
    "\n",
    "\n",
    "# for hugging face space authentication to upload files\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Set scikit-learn's display mode to 'diagram' for better visualization of pipelines and estimators\n",
    "sklearn.set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "51b91836"
   },
   "source": [
    "# **Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "YckcdzTlp839"
   },
   "outputs": [],
   "source": [
    "# Load the dataset of SuperKart and make a copy for EDA Analysis followed by Model building.\n",
    "skart_ori = pd.read_csv(\"SuperKart.csv\")\n",
    "\n",
    "# skart_ori = pd.read_csv(\"/content/SuperKart.csv\")\n",
    "skart = skart_ori.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "da9e2230"
   },
   "source": [
    "- Load dataset and make a copy\n",
    "- Use the copy for EDA analysis and Model building.\n",
    "- EDA Analysis will add more columns where necessary (Feature Engineering)\n",
    "- Model building will perform scaling using RobustScaler on this dataset.\n",
    "- As it safe not to modify the orginal copy, a copy is taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "W2sXwrUERYua"
   },
   "source": [
    "# **Data Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "1a42a628"
   },
   "source": [
    "## **Display the first 5 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "f7a288ec",
    "outputId": "4d5f2f77-c678-4427-cfcb-bdf8380a711c"
   },
   "outputs": [],
   "source": [
    "skart.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "98743f0a"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Dataset is loaded properly\n",
    "- First 5 rows are displayed correctly as verfied in CSV file manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "e05613cc"
   },
   "source": [
    "## **Display the dimension of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03beed67",
    "outputId": "a1f3d207-8067-4a18-96fb-293e8a7f3398"
   },
   "outputs": [],
   "source": [
    "print ( skart.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "cdc5fb0d"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- There are 8763 rows in the dataset\n",
    "- There are 12 columns in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "8a7dc627"
   },
   "source": [
    "## **Display the column names and data types**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "21d98f22"
   },
   "source": [
    "### **Dataframe info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e076ac6",
    "outputId": "581a145a-9f7e-4a8c-a5b9-a6616354759f"
   },
   "outputs": [],
   "source": [
    "skart.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "54676742"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Dataset is no null values.\n",
    "- It has 5 Numeric fields 7 object fields. These fields are categorical in nature.They have to be converted.\n",
    "- Product ID and Store ID are the main idenitifers that uniquely idenitfy the products and stores respectively.\n",
    "- Products have numeric attributes like Weight, Allocated Area and MRP. Also, products addtionally have two Categorical attributes, sugar content and type of the product.\n",
    "- Stores have only Categorical attributes like Type, Size, location and year of establishment.\n",
    "- Product_Store_Sales_Total is the target variable. It is having sales recorded for each product and Store combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "c2d55495"
   },
   "source": [
    "### **Create aliases and classify numeric and categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "id": "e2ad6768"
   },
   "outputs": [],
   "source": [
    "# Define the total sales\n",
    "target = 'Product_Store_Sales_Total'\n",
    "\n",
    "# Use short form aliases to avoid typos\n",
    "pid = 'Product_Id'\n",
    "sid = 'Store_Id'\n",
    "pwt = 'Product_Weight'\n",
    "paa = 'Product_Allocated_Area'\n",
    "pmrp = 'Product_MRP'\n",
    "seyr = 'Store_Establishment_Year'\n",
    "psc = 'Product_Sugar_Content'\n",
    "ptyp = 'Product_Type'\n",
    "ssize = 'Store_Size'\n",
    "sloctype = 'Store_Location_City_Type'\n",
    "styp = 'Store_Type'\n",
    "\n",
    "# Deine numeric variables - Include Target in Numeric variables as it continuous variable and not classification.\n",
    "numeric_vars = [pwt,paa,pmrp,target]\n",
    "\n",
    "# Even though 'Store_Establishment_Year' is Numeric lets consider it as Categorical.\n",
    "# Define categorical variables\n",
    "categories = [psc,ptyp,ssize,sloctype,styp,seyr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "cce54787"
   },
   "source": [
    "## **Display the statistical summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rtomleOyp8dQ",
    "outputId": "d3efd8e7-0c0c-461e-dcdc-6d7971dbb738"
   },
   "outputs": [],
   "source": [
    "skart.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "1dcf9072"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "\n",
    "There are 5 Numeric variables:\n",
    "\n",
    "- **Product_Weight:** the units are not mentioned. The assumption is that it is in oz. The weight ranges from 4 to 22.\n",
    "\n",
    "- **Product_Allocated_Area:** As per definition this ratio is specific to the stores and it is the ratio of display area of product to total display area. The min is 0.004 and max is almost 0.298. There are no product Names. Only Product Type and categories are available. The mean is 0.056 suggesting that it is right skewed data. Most products have a shorter display area even though there sales might be higher. It could be items are smaller in size and may require smaller area to fit most of the items.\n",
    "\n",
    "\n",
    "- **Product_MRP:** The currency is not Specified. It is assumed to be US dollars. The Minimum price is 31 and maximum 266.\n",
    "\n",
    "- **Establishment year:** The first store was established in 1987 and most latest one is in 2009.  \n",
    "\n",
    "- **Product_Store_Sales_Total:** This is sales for the product and Store combination. It is the Target variable to be predicted. Min sales per product per stores is 33 and max is 8000.  \n",
    "\n",
    "- There are no NULL values  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "4oamAwxrVHLq"
   },
   "source": [
    "# **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "abcd0558"
   },
   "source": [
    "## **Helper functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "433359bb"
   },
   "source": [
    "### **Visualizing Univariate data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "132e0a86"
   },
   "outputs": [],
   "source": [
    "# Helper function for checking if there are any Null or missing values in the data.\n",
    "def col_vals(data,c):\n",
    "    print(c)\n",
    "    # print(data[c].unique())\n",
    "    # print(data[c].value_counts())\n",
    "    print(\"Check Missing values for \", c ,\" = \",\n",
    "          \"No missing values \" if ( ( data[c].value_counts().sum() - data.shape[0] == 0) and (data[c].isnull().sum()/len(data)*100 == 0) ) else \" invalid entires may exist\")\n",
    "    print('---------------------------------')\n",
    "\n",
    "# Helper Function for visualizing a feature and the occurance. It gives density and frequency\n",
    "def countplot_feature(data, feature, figsize=(12, 7), color='skyblue', rotation=0, nrows=2):\n",
    "    \"\"\"\n",
    "    Count plot for categorical features\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    color: color of bars (default blue)\n",
    "    rotation: rotation of x-axis labels (default 0)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, ft in enumerate(feature):\n",
    "        plt.subplot(nrows, 2, i+1)\n",
    "        plt.title(f'counts of {ft} categories')\n",
    "        sns.countplot(x=data[ft], color=color)\n",
    "        plt.xticks(rotation=rotation)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# This function is used only if a feature is existing in two data sets and it needs to be compared.\n",
    "# For ex: The difference of distribution of a feature n Test and Training datasets.\n",
    "def histplot_compare(data1, data2, feature):\n",
    "    # cols = cols /2\n",
    "    total = len(feature)\n",
    "    rows = int( total)\n",
    "    bins1 = int( data1.shape[0]/100 )\n",
    "    bins2 = int( data2.shape[0]/25 )\n",
    "    fig, ax_hist = plt.subplots(nrows=rows, ncols=2, figsize=(8,75))\n",
    "    # print (ax_hist)\n",
    "    for i, ft in enumerate(feature):\n",
    "        ax_hist1 = ax_hist[i, 0]\n",
    "        ax_hist2 = ax_hist[i, 1]\n",
    "        ax_hist1.sharex(ax_hist2)\n",
    "        # ax_hist1.sharey(ax_hist2)\n",
    "        # plt.title(f'{ft} histogram')\n",
    "        pcolor=sns.color_palette('Set1_r', as_cmap = True)\n",
    "        # Palette = [\"#090364\", \"#091e75\"] #define your preference\n",
    "        # sns.set_style(\"whitegrid\")\n",
    "        # sns.set_palette(Palette) #use the list defined in the function\n",
    "        ax1 = sns.histplot(\n",
    "            data=data1, x=ft, kde=True, ax=ax_hist1, bins=bins1, palette=pcolor, edgecolor=None\n",
    "        )\n",
    "        ax2 = sns.histplot(\n",
    "            data=data2, x=ft, kde=True, ax=ax_hist2, bins=bins2, palette=pcolor, edgecolor=None\n",
    "        )\n",
    "        ax_hist2.axvline(\n",
    "            data2[ft].mean(), color=\"crimson\", linestyle=\"solid\"\n",
    "        )  # Add mean to the histogram\n",
    "        ax_hist2.axvline(\n",
    "            data2[ft].median(), color=\"darkgreen\", linestyle=\"solid\"\n",
    "        )  # Add median to the histogram\n",
    "        ax_hist1.axvline(\n",
    "            data1[ft].mean(), color=\"crimson\", linestyle=\"solid\"\n",
    "        )  # Add mean to the histogram\n",
    "        ax_hist1.axvline(\n",
    "            data1[ft].median(), color=\"darkgreen\", linestyle=\"solid\"\n",
    "        )  # Add median to the histogram\n",
    "        ax1.lines[0].set_color('black'); ax2.lines[0].set_color('black')\n",
    "        ax1.set_xlabel(ft  + \"(Training)\"); ax1.set_ylabel('Frequency')\n",
    "        ax2.set_xlabel(ft + \"(Testing)\"); ax2.set_ylabel('Frequency')\n",
    "        plt.xticks(rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# This function is used for plotting a histogram with Mean and Median.\n",
    "def histplot_feature(data, feature, cols=4, figsize=(12,7), fth=None):\n",
    "    total = len(feature)\n",
    "    rows = int( total / cols)\n",
    "    bins = 20\n",
    "    fig, ax_hist = plt.subplots(nrows=rows, ncols=cols, figsize=figsize)\n",
    "    # print (ax_hist)\n",
    "    for i, ft in enumerate(feature):\n",
    "        ax_hist2 = ax_hist[int(i/cols), i%cols]\n",
    "        # plt.title(f'{ft} histogram')\n",
    "        ax = sns.histplot(\n",
    "            data=data, x=ft, kde=True, ax=ax_hist2, palette='Set1', bins=bins, edgecolor=None, hue=fth\n",
    "        )\n",
    "        ax_hist2.axvline(\n",
    "            data[ft].mean(), color=\"crimson\", linestyle=\"solid\"\n",
    "        )  # Add mean to the histogram\n",
    "        ax_hist2.axvline(\n",
    "            data[ft].median(), color=\"darkgreen\", linestyle=\"solid\"\n",
    "        )  # Add median to the histogram\n",
    "        ax.lines[0].set_color('black')\n",
    "        plt.xticks(rotation=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# This function is used for plotting a Box Plot on a features showing quartiles and Outiliers\n",
    "def boxplot_feature(data, feature, cols=4, figsize=(12,7), fth=None):\n",
    "\n",
    "    total = len(feature)\n",
    "    rows = int( total / cols)\n",
    "    fig, ax_box = plt.subplots(nrows=rows, ncols=cols, figsize=figsize)\n",
    "    for i, ft in enumerate(feature):\n",
    "        ax_box2 = ax_box[int(i/cols), i%cols]\n",
    "        # plt.title(f'{ft} boxplot')\n",
    "        ax = sns.boxplot(\n",
    "            data=data, x=ft, showmeans=True, ax=ax_box2, palette='Set2', hue=fth\n",
    "        )\n",
    "        ax.legend().remove()\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1,1) )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Similar to Histplot but show the KDE line.\n",
    "def kdeplot_feature(data, feature, figsize=(12, 7), color='skyblue', rotation=0):\n",
    "    \"\"\"\n",
    "    kdeplot for categorical features\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    color: color of bars (default blue)\n",
    "    rotation: rotation of x-axis labels (default 0)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, ft in enumerate(feature):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.title(f'kde {ft} distribution')\n",
    "        sns.kdeplot(x=data[ft], color=color)\n",
    "        plt.xticks(rotation=rotation)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# function to plot a boxplot and a histogram along the same scale.\n",
    "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    kde: whether to show the density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram\n",
    "\n",
    "    # function to create labeled barplots\n",
    "\n",
    "# This function is wrpper function for a barplot.\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n],\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "59171de0"
   },
   "source": [
    "### **Categorical data - Label/Ordinal Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "6e9ebadb"
   },
   "outputs": [],
   "source": [
    "def ordinal_enc( data, col, cat, drop=False):\n",
    "    \"\"\"\n",
    "    Function to perform ordinal encoding on the specified columns\n",
    "\n",
    "    data: dataframe\n",
    "    col: column to be encoded\n",
    "    drop: whether to drop the column after encoding (default is False)\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "    enc = OrdinalEncoder(categories=cat)\n",
    "    data[col+'_enc'] = enc.fit_transform(data[[col]])\n",
    "    if drop:\n",
    "        data.drop(columns=[col], inplace=True)\n",
    "\n",
    "def label_enc( data, col, drop=False):\n",
    "    \"\"\"\n",
    "    Function to perform ordinal encoding on the specified columns\n",
    "\n",
    "    data: dataframe\n",
    "    col: column to be encoded\n",
    "    drop: whether to drop the column after encoding (default is False)\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    enc = LabelEncoder()\n",
    "    data[col+'_enc'] = enc.fit_transform(data[[col]])\n",
    "    if drop:\n",
    "        data.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "3389906f"
   },
   "source": [
    "### **Visualizing Bi-Variate data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "id": "d7f329b2"
   },
   "outputs": [],
   "source": [
    "# Plot two Categorical variable on X and Y axis.\n",
    "def cross_tab(data, feature1, feature2, nrows=1, ncols=2, figsize=(15,5), stacked=True, margins=True):\n",
    "    \"\"\"\n",
    "    Cross tabulation of two features\n",
    "\n",
    "    df: dataframe\n",
    "    feature1: first feature\n",
    "    feature2: second feature\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    tab1 = pd.crosstab(data[feature1], data[feature2], dropna=False)\n",
    "    if ( margins ):\n",
    "        tab2 = pd.crosstab(data[feature1], data[feature2], normalize='index', margins=True, margins_name=\"Total\", dropna=True)\n",
    "    else:\n",
    "        tab2 = pd.crosstab(data[feature1], data[feature2], normalize='index', margins=False, dropna=True)\n",
    "    # print(tab1)\n",
    "    # print(tab2)\n",
    "    # print(type(fig) )\n",
    "    fig.set_size_inches(figsize)\n",
    "    xl = feature1\n",
    "    yl = \"Proportion of \" + feature2\n",
    "    tit = \" Actual counts\"\n",
    "    tab1.plot(kind='bar', ax=axes[0], stacked=stacked, title=tit, xlabel=xl, ylabel=yl)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(0,0), title=feature2)\n",
    "\n",
    "    tit = \" Normalized (individual) counts\"\n",
    "    tab2.plot(kind='bar', ax=axes[1], stacked=stacked, title=tit, xlabel=xl, ylabel=yl)\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(1,1), title=feature2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# seperate the data based on unit_of_wage (espically hour) because the values of hours is low compared to other (monthly, yearly and week).\n",
    "# Putting everything to together will make the hour plot invisible. As we do not know if hours is 8 or 12 hours etc.\n",
    "# It is better to seperate categories and look at prevailing_wage trend in the category.\n",
    "#  Also as identified in the corr matirx the other influencers of `previaling_wage` are `full_time_position`, `continent` and `region_of_employment.\n",
    "# While analyzing the variable of `prevailing_wage` and its influencers we keep an eye of the overall impact on `case_status`\n",
    "visa_palette = {0: \"#fc8d62\", 1: \"#66c2a5\", \"test\": \"#66c2a5\", \"train\": \"#fc8d62\", \"validation\": \"#ffd92f\",\"testing\": \"#66c2a5\", \"training\": \"#fc8d62\",\n",
    "                'full': '#ffd92f', 'mean': '#a6d854', 'sum':'#66c2a5', 'std':'#66c2a5', 'min': '#8da0cb',\n",
    "                \"max\": \"#fc8d62\", \"median\": \"#fc8d62\", \"count\": \"#ffd92f\",\n",
    "                paa: \"#66c2a5\", pwt: \"#fc8d62\", pmrp: \"#ffd92f\",target:'#8da0cb',pid:'#a6d854',\n",
    "                \"25%\": \"#66c2a5\", \"50%\": \"#fc8d62\", \"75%\": \"#ffd92f\",\n",
    "                'F1': '#fc8d62', 'Accuracy': '#a6d854', 'Recall':'#66c2a5', 'Precision': '#8da0cb',\n",
    "                'Regular': '#fc8d62', 'No Sugar': '#a6d854', 'Low Sugar': '#66c2a5', 'reg': '#8da0cb',\n",
    "                'NC':\"#66c2a5\", 'FD':\"#fc8d62\", 'DR':\"#ffd92f\"\n",
    "                }\n",
    "# Format axis ticks\n",
    "def custom_fmt(x):\n",
    "    return f'{x/1000:.1f}K' if x >= 10000 else f'{x:.2f}'\n",
    "\n",
    "# Formatting helper function used by all Fecet Grid plot for handling common display issues.\n",
    "def fmt_plot(g, nrows, ncols, row_d, col_d, hue_d, x_d, y_d, fnt, tit, kind='None'):\n",
    "    print(f\"5-dimension analysis: {row_d} X {col_d} = {nrows} X {ncols} with hue = {hue_d} on variables {x_d} over {y_d}\")\n",
    "    length = len(g.axes_dict)\n",
    "    print ( length, g.axes_dict.keys())\n",
    "    print ( length, g.axes_dict.items())\n",
    "    if (tit):\n",
    "        #  g.set_titles(col_template=\"{col_var}={col_name}\", row_template=\"{row_var}={row_name}\", size=fnt)\n",
    "        g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\", size=fnt)\n",
    "    for i,m in enumerate(g.axes_dict):\n",
    "        ax = g.axes_dict[m]\n",
    "        # if(not tit):\n",
    "        ax.tick_params(direction='out', axis='x', labelsize=fnt, colors='b', grid_color='r', labelrotation=90)\n",
    "        ax.set_xlabel(str(x_d) + \"(\" + row_d + \"=\"+ str(m[0]) + \")\", fontsize=fnt)\n",
    "        ax.tick_params(direction='out', axis='y', labelsize=fnt, colors='b', grid_color='r')\n",
    "        ax.set_ylabel(str(y_d) + \"(\" + col_d + \"=\"+str(m[1]) + \")\", fontsize=fnt)\n",
    "        # else:\n",
    "        #     ax.tick_params(direction='out', axis='x', labelsize=fnt, colors='b', grid_color='r', labelrotation=90)\n",
    "        #     ax.set_xlabel(str(x_d), fontsize=fnt)\n",
    "        #     ax.tick_params(direction='out', axis='y', labelsize=fnt, colors='b', grid_color='r')\n",
    "        #     ax.set_ylabel(str(y_d), fontsize=fnt)\n",
    "        if (kind =='bar'):\n",
    "            for bars in ax.containers:\n",
    "                ax.bar_label(bars, labels=[custom_fmt(v) for v in bars.datavalues], color='red',\n",
    "                              padding=3, fontsize=fnt)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function for controlling the axis ranges on the map.\n",
    "# In FacetGrid it helos to have same Y-axis ticks for better comparision.\n",
    "def unify_axes(data, g, row_d, col_d, hue_d, x_d, y_d, kind=None):\n",
    "    nrows =data[row_d].nunique()\n",
    "    ncols = data[col_d].nunique()\n",
    "    # if( hue_d):\n",
    "    g.add_legend(title=hue_d, loc='center', bbox_to_anchor=(0,0))\n",
    "    # Find global y-limits across all axes\n",
    "    ymin, ymax = data[y_d].min(), data[y_d].max()*1.2\n",
    "    # Apply same y-limits to all subplots\n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "        if ( ymax > 1000):\n",
    "            ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x/1000:.1f}K'))\n",
    "        else:\n",
    "            ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f'{x:.1f}'))\n",
    "        if(kind =='bar'):\n",
    "            for bar in ax.patches:\n",
    "                bar.set_x(bar.get_x() + 0.2)  # Shift right by 0.1 units\n",
    "    return nrows, ncols\n",
    "\n",
    "# This function renders a Heat Map inside a Facet Grid.\n",
    "# As we cannot use map dataframe for heat map,\n",
    "# this function uses the Facet ax for controlling the rendering of each Heatmap of the Facet.\n",
    "def FHMap (data, row_d, col_d, vars, fnt=8, ht=None, asp=None, tit=False, annot_fnt=8,debug=False):\n",
    "    nrows = data[row_d].nunique()\n",
    "    ncols = data[col_d].nunique()\n",
    "\n",
    "    if( (ht is None) | (asp is None) ):\n",
    "        # Decide the default Facet cell size.\n",
    "        hgt_1f = 2.75;\n",
    "        asp = 1.25\n",
    "        # Compute other sizes.\n",
    "        wdt_1f = round(hgt_1f*asp,2); fig_h = round(nrows*hgt_1f,2); fig_w = round(ncols * wdt_1f, 2);\n",
    "        ht = hgt_1f;\n",
    "\n",
    "    # Plot two heatmaps - first one will be all variables together with annotation and FacetGrid.\n",
    "    # first heatmap is a single one. Calculate figure size using\n",
    "    mapsize = len(vars)\n",
    "    map_width = int(round(wdt_1f * mapsize,0))\n",
    "    map_height = int(round(hgt_1f * mapsize))\n",
    "\n",
    "    if(debug):\n",
    "        print(f\"Total height={fig_h}, Total width={fig_w}, FG-cell height={ht}, FG-Cell width={round(asp*ht,2)}, \\\n",
    "              FG-cell aspect={asp}, ref. heatmap width{map_width}:height{map_height}\")\n",
    "\n",
    "    plt.figure(figsize=(map_width,map_height))\n",
    "    heat_data = data.loc[:, vars].corr()\n",
    "    # print ( len(heat_data), type(heat_data))\n",
    "    ax = sns.heatmap(heat_data, annot=True, cbar=True,\n",
    "                vmin=-1,vmax=1,fmt='.1f', cmap=\"Spectral\", annot_kws={\"size\": 15},\n",
    "                xticklabels=True, yticklabels=True)\n",
    "    ax.set_title(f\"Heatmaps using: {' | '.join(vars)} in the dataset.\\n\\n\\\n",
    "    (1) The first Heatmap does not have slicing Facets of {row_d} and {col_d}\\n\\\n",
    "    (2) Compare this with the Grid of Heatmap Facets below which uses Facets of {row_d} and {col_d}\\n\\\n",
    "    (3) All heatmaps use the same set of features in the same order\\n\\\n",
    "    (4) The first heatmap acts as reference heatmap to show all the features names involved\\n\\\n",
    "    (5) The columns are abbreviated to reduce space occupied on the graph.\\n\",\n",
    "    loc='left', color='red')\n",
    "\n",
    "    ax.tick_params(axis='x', rotation = 90)\n",
    "    ax.tick_params(axis='y', rotation =0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Second heatmap will be inside FacetGrid every Facet split by nrows and ncols will have a heatmap.\n",
    "    # This way one can compare if the data seperation is influencing the correlation.\n",
    "    g = sns.FacetGrid(data, row=row_d, col=col_d, height=ht, aspect=asp, sharex=False, sharey=False)\n",
    "    for (r_val, c_val), ax in g.axes_dict.items():\n",
    "        data_sub = data[(data[row_d] == r_val) & (data[col_d] == c_val)]\n",
    "        heat_data = data_sub.loc[:, vars]\n",
    "        # print(f\"Handing Facet {r_val} and {c_val} using {row_d} x {col_d}, Heatmap features = {vars}. Length of data for correlation {len(heat_data)}\")\n",
    "        sns.heatmap(heat_data.corr(), ax=ax, cbar=False, annot=True, annot_kws={\"size\": annot_fnt},\n",
    "                    xticklabels=True, yticklabels=True,  vmin=-1,vmax=1,fmt='.1f', cmap=\"Spectral\")\n",
    "    fmt_plot(g, nrows, ncols, row_d, col_d, None, \"All Numeric Vars\", vars[-1], fnt, tit)\n",
    "\n",
    "# Uses Facet Grid for rendering Bar plot for each Facet in the Grid.\n",
    "def FBar ( data, row_d, col_d, hue_d, x_d, y_d, fnt=8, ht=2.5, asp=1.5, tit=False, bars=True, cold=None, hue_order=None):\n",
    "    if (hue_order is None):\n",
    "        hue_order = sorted(data[hue_d].dropna().unique())\n",
    "    if (cold is None):\n",
    "        cold = sorted(data[col_d].dropna().unique())\n",
    "    g = sns.FacetGrid(data, row=row_d, col=col_d, height=ht, aspect=asp, sharex=False, sharey=False, col_order=cold)\n",
    "    g.map_dataframe(sns.barplot, x=x_d, y=y_d, hue=hue_d, dodge=True, palette=visa_palette, hue_order=hue_order, ci=None)\n",
    "    nrows, ncols = unify_axes(data, g, row_d, col_d, hue_d, x_d, y_d, kind='bar')\n",
    "    fmt_plot(g, nrows, ncols, row_d, col_d, hue_d, x_d, y_d, fnt,tit, kind=('bar' if (bars) else 'None') )\n",
    "\n",
    "# Uses Facet Grid for rendering Box plot for each Facet in the Grid.\n",
    "def FBox ( data, row_d, col_d, hue_d, x_d, y_d, fnt=8, ht=2.5, asp=1.5, tit=False, cold=None):\n",
    "    hue_order = sorted(data[hue_d].dropna().unique())\n",
    "    if (cold is None):\n",
    "        cold = sorted(data[col_d].dropna().unique())\n",
    "    g = sns.FacetGrid(data, row=row_d, col=col_d, height=ht, aspect=asp, sharex=False, sharey=False, col_order=cold)\n",
    "    g.map_dataframe(sns.boxplot, x=x_d, y=y_d, hue=hue_d, palette=visa_palette, hue_order=hue_order) #.add_legend()\n",
    "    nrows, ncols = unify_axes(data, g, row_d, col_d, hue_d, x_d, y_d)\n",
    "    fmt_plot(g, nrows, ncols, row_d, col_d, hue_d, x_d, y_d, fnt,tit)\n",
    "\n",
    "# Uses Facet Grid for renderinganchor Violin plot for each Facet in the Grid.\n",
    "def FViolin ( data, row_d, col_d, hue_d, x_d, y_d, fnt=8, ht=2.5, asp=1.5, tit=False, cold=None):\n",
    "    hue_order = sorted(data[hue_d].dropna().unique())\n",
    "    if(cold is None):\n",
    "        cold = sorted(data[col_d].dropna().unique())\n",
    "    g = sns.FacetGrid(data, row=row_d, col=col_d, height=ht, aspect=asp, sharex=False, sharey=False, col_order=cold)\n",
    "    g.map_dataframe(sns.violinplot, x=x_d, y=y_d, hue=hue_d, palette=visa_palette, hue_order=hue_order)\n",
    "    nrows, ncols = unify_axes(data, g, row_d, col_d, hue_d, x_d, y_d)\n",
    "    fmt_plot(g, nrows, ncols, row_d, col_d, hue_d, x_d, y_d, fnt,tit)\n",
    "\n",
    "# Uses Facet Grid for rendering Scatter plot for each Facet in the Grid.\n",
    "def FScatter( data, row_d, col_d, hue_d, x_d, y_d, fnt=8, ht=2, asp=3, tit=False, cold=None):\n",
    "    hue_order = sorted(data[hue_d].dropna().unique())\n",
    "    if(cold is None):\n",
    "        cold = sorted(data[col_d].dropna().unique())\n",
    "    g = sns.FacetGrid(data, row=row_d, col=col_d, height=ht, aspect=asp, sharex=False, sharey=False, col_order=cold)\n",
    "    g.map_dataframe(sns.scatterplot, x=x_d, y=y_d, hue=hue_d, palette=visa_palette, hue_order=hue_order)\n",
    "    nrows, ncols = unify_axes(data, g, row_d, col_d, hue_d, x_d, y_d)\n",
    "    fmt_plot(g, nrows, ncols, row_d, col_d, hue_d, x_d, y_d, fnt,tit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "7a1460a2"
   },
   "source": [
    "## **Correct inconsistent and invalid data in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "95d410c5"
   },
   "source": [
    "### **Check for missing or null values. Check for Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5a9a386",
    "outputId": "7adad57a-6b7f-44c7-830b-d0f4c8b1dc3e"
   },
   "outputs": [],
   "source": [
    "#Check for null values.\n",
    "for c in skart.columns:\n",
    "    col_vals(skart, c)\n",
    "#check for Duplicates\n",
    "print ( \"Duplicate rows = \", \"No duplicates\" if (skart.duplicated().value_counts()[0] == skart.shape[0]) else \" Duplicate rows exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "id": "4d7ab809"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- None of columns are having NULL values or missing values.\n",
    "- None of values are negative\n",
    "- No duplicates in the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "56a215d8"
   },
   "source": [
    "### **Check Categorical variables for invalid entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e8c16ee5",
    "outputId": "94776b8c-a542-4aff-94b9-ec7b65cc4cb3"
   },
   "outputs": [],
   "source": [
    "vcdf = pd.DataFrame ( [{ 'Var': x, 'Total': skart[x].nunique(), 'Category': yind, 'Count': yval } for x in categories for yind, yval in skart[x].value_counts().items() ] )\n",
    "vcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "c5270ec7"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "\n",
    "There are 6 categorical variables (The 6th one \"Store_Establishment_Year\" was a numeric variable explained above. It is considered as categorical for better analysis).\n",
    "\n",
    "- **Product Type:** Has 16 types of Products lead by \"Frutis and Vegetables\" and with \"Seafood\" at the bottom. There is a Household category of items, apart form this all other items are perishable food items.   \n",
    "\n",
    "- **Store Type:** There are 4 Stores with \"SuperMarket Type2\" being the store with highest number of products sold.  \n",
    "\n",
    "- **Store_Loaction_City_Type:** It has 3 classifications \"Tier 1\", \"Tier 2\" and \"Tier 3\". \"Tier 1\" has a high cost of living. However, the sales are higher in \"Tier 2\" compared the others.  \n",
    "\n",
    "- **Store_Establishment_Year:** It has the Year as a Numeric. The lowest is 1987 and highest 2009. It shows a newer location (probably with better ambience and more like Mall) is attracting more sales than the older once (probably with a more classic look).  \n",
    "\n",
    "- **Store Size:** The Sizes are classified as \"Small\", \"Medium\" and \"High\". The Medium is having maximum sales compared to High and Small.  \n",
    "\n",
    "- **Product_Sugar_Content:** It has 3 classifications as per the documentation. \"Low Sugar\", \"Regular\" and \"No Sugar\". There is an extra category \"reg\" in the data. The number of this are very low. It must be corrected and treated as \"Regular\". \"Low Sugar\" products are recording highest sales.\n",
    "\n",
    " - There are two Ids Product_Id and Store_Id. They uniquely identify products and stores respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "d84e04eb"
   },
   "source": [
    "### **Correct the invalid data present in the Product_Sugar_Content column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab56bf77",
    "outputId": "f997b485-7bd2-465c-eb10-257bd05d8d5b"
   },
   "outputs": [],
   "source": [
    "print (\"Before replacement\",skart[psc].value_counts() )\n",
    "skart[psc] = np.where(skart[psc]==\"reg\", \"Regular\", skart[psc])\n",
    "print (\"\\n\\nAfter replacement\",skart[psc].value_counts() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "id": "ddbb4f5a"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Now the values of Product_Sugar_Content are converted from 4 categories to 3 categories which matches the specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "id": "8e9b34c4"
   },
   "source": [
    "## **Basic Analysis of the dataset**\n",
    "- Dataset is complicated and many relationship posibilties of analysis\n",
    "- Therefore get a good understanding of the basic relationships first\n",
    "- After the basic relationships are understood we will do deeper Univariate, Bivariate and Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "d3975382"
   },
   "source": [
    "### **Understanding Stores data in the dataset**\n",
    "- Understand the vairables Store_Id, Store_Size, Store_Location_City_Type, Store_Establishment Year and Store_Type\n",
    "- Keep Product_Store_Sales_total (Target Variable) also in the analysis to get basic relationship with Target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "id": "5f81cf05"
   },
   "source": [
    "#### **How many stores are there ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "1255d7b9",
    "outputId": "ec671257-1883-4126-9cd5-8c33ec73d668"
   },
   "outputs": [],
   "source": [
    "## Get all the stores\n",
    "print( \"Number of Stores = \", skart[sid].nunique() )\n",
    "stores = skart.loc[:, [sid, ssize, sloctype, seyr, styp, target] ]\n",
    "stores = stores.groupby([sid]).agg( {\n",
    "    target:'sum',\n",
    "    ssize:'first',\n",
    "    sloctype:'first',\n",
    "    seyr: 'first',\n",
    "    styp: 'first'\n",
    "    }).reset_index()\n",
    "stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "id": "f108e62b"
   },
   "source": [
    "#### **Store with highest sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a564ecdb",
    "outputId": "3507bb9f-f91d-401e-d65f-e78e17346bec"
   },
   "outputs": [],
   "source": [
    "smax = stores[target].idxmax()\n",
    "print ( stores.loc[smax,:] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "id": "c1c76cd3"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- There are 4 Stores each of these Stores is uniquely identified by Store_Type. Store_id is therefore redundant. The Store with id \"OUT004\" is having highest sales.\n",
    "\n",
    "- The \"Tier 2\" \"Medium\" size Stores is doing well. It recorded the maximum revenue. About 250% of the \"High\" size store. It was established in 2009 as apposed to other stores which atleast a decade earlier than this.\n",
    "   - THIS SHOWS THE LATEST ESTABLISHED STORES IS PROBABLY HAVING BETTER FACILITIES AND/OR TECH/AMBIENCE WHICH IS ATTRACTING THE CUSTOMERS MORE.\n",
    "   - THE BUSINESS MUST TAKE NOTE OF THIS AND CONSIDER RENOVATION.\n",
    "- Departmental Store and Supermarket Type1 have almost equals revenues. Even though the Size of the SuperMarket Type1 Store is the oldest store (1987) and is size category \"High\", it has lesser revenue compared to Departmental Store of \"Medium\" size. However, the Departmental Store is in Tier and SuperMarket Type1 is in Tier 2 which might be reason for lower sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "id": "bb8b6cdb"
   },
   "source": [
    "### **Understanding Products and Products Vs Stores relationships in the dataset**\n",
    "- Understand Product Type variable. Its relationship with Stores\n",
    "- Understand Product_Allocation_Area, Product_Weight and Product_MRP.\n",
    "- Understand Product Vs Stores relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "id": "262e9a54"
   },
   "source": [
    "#### **Number of Products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffc93d72",
    "outputId": "75aef255-c19e-4f47-adae-9779063badab"
   },
   "outputs": [],
   "source": [
    "print( \"Number of Products = \", skart[pid].nunique())\n",
    "products = skart.loc[:, [pid, ptyp, paa, pmrp, pwt, psc, target] ]\n",
    "print( len(products), products.duplicated().value_counts() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "id": "242fb3f9"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- There are 8763 products in total with unique Product ID.\n",
    "- In the dataset no Product ID is repeated twice. It means dataset has already aggregated the data w.r.t to Product IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "id": "1fd5b378"
   },
   "source": [
    "#### **Understanding Product Weight**\n",
    "- For any analysis going forward the data will be sliced into Product Types and Store Types\n",
    "- There are distinct patterns that will noted for all the variables with combination or Ptoduct Type and Store Types\n",
    "- There are 16 Product types and 4 Store types. A total of 64 combinations.\n",
    "- The pie chart below aggregates a mean average for the product Weights across all product and Store combinations.\n",
    "- the pie chart has 16 combinations of product and each column is 4 piechart one for each Store.\n",
    "- The first column has Mean Weight and second column has the sum of Area.\n",
    "- **The pie chart will tell - in the overall sales (calssified per product and store type combination):**\n",
    "    - How much was average Weight of the products that was sold in that product type per Store ?\n",
    "    - how much was the total Area that was occupied by the sold items ?\n",
    "\n",
    "**NOTE: The Total Area is a ratio and not a true number. So it will reflect relative sales between product items but not give absolute values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ab1ab76a",
    "outputId": "962e2c31-cf20-4dd1-931a-5c2190010393"
   },
   "outputs": [],
   "source": [
    "pskw = skart.groupby([styp,ptyp]).agg({pwt:'mean', paa:'sum', pid:'count'}).unstack(level=0)\n",
    "pskw.columns = [y+(\" Weight\" if x == pwt else \" Pid\" if x == pid else \" Area\") for x,y in pskw.columns]\n",
    "\n",
    "ctyp = \"Dummy\"\n",
    "def autopct_format_factory(df_vals):\n",
    "\tdef autopct_format(pct, all_vals=df_vals):\n",
    "\t\t\t\tautopct_format.counter += 1\n",
    "\t\t\t\tidx = autopct_format.counter - 1\n",
    "\t\t\t\t# lis = list(skart[styp].unique())\n",
    "\t\t\t\tcolnm = ctyp+\" Pid\"\n",
    "\t\t\t\tabs_val = all_vals.iloc[idx][colnm]  # pick the column you want\n",
    "\t\t\t\treturn f\"{pct:.1f}%\\n({abs_val})\"\n",
    "\tautopct_format.counter = 0\n",
    "\treturn autopct_format\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(4,2,figsize=(30,50) )\n",
    "ax = ax.flatten()\n",
    "\n",
    "for n,i in enumerate(skart[styp].unique()):\n",
    "\t\t\tctyp = i\n",
    "\t\t\tax1 = ax[2*n]\n",
    "\t\t\tax2 = ax[2*n+1]\n",
    "\t\t\ty1c = i+\" Weight\"\n",
    "\t\t\ty2c = i+\" Area\"\n",
    "\t\t\ty3c = i+\" Pid\"\n",
    "\t\t\t# print (i, y1c,y2c)\n",
    "\t\t\t# ax1.pie(pskw[y1c], labels=pskw.index, autopct='%1.1f%%')\n",
    "\t\t\tax1.pie(pskw[y1c], labels=pskw.index, autopct=autopct_format_factory(pskw), textprops={'fontsize': 14}, radius=1.1)\n",
    "\t\t\tax1.set_title(y1c+\" AVG SUM = \"+str( round(pskw[y1c].sum(),3))+\" (Orders= \"+str(pskw[y3c].sum()) +\")\", fontsize=20, fontweight='bold')\n",
    "\t\t\tax2.pie(pskw[y2c], labels=pskw.index, autopct='%1.1f%%',   textprops={'fontsize': 14}, radius=1.1)\n",
    "\t\t\tax2.set_title(y2c+\" SUM = \"+str(round(pskw[y2c].sum(),3)), fontsize=20, fontweight='bold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {
    "id": "6827c3aa"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- The pie chart on left shows the SUM of AVG means of Product Weight across unique combinations of Product Type and Store Type.\n",
    "\n",
    "- The pie chart on the left clearly shows that sum of Means is higher for Depratmental Store. This means Depratmental stores is selling product that are having higher Weights. It could be products with higher MRP and/or Premium products. Could be that Multiple numbers( 5 pack or 10 pack) products are bundles together and sold. This is the reason for higher weights and therefore higher sum of means.\n",
    "\n",
    "- The SuperMarket Type2 we already saw has highest sales. Here we see that it has a lower sum of means. However it is still higher that the Food Market. So the food Market is selling products that more cheaper probably lower prices. Not the Premium range of products. SuperMarket Type2 is selling smaller to Average sizes. This could also be a potential reason on why sales are high because custoemrs may not prefer large/Premium sizes always, and having options between low to average sizes might be preferred on a regular prices.\n",
    "\n",
    "- SuperMarket Type1 is unique here. As we saw already it is vintage store from 1987. What we see here is that its weights are closer to Departmental store and significantly higher than its Type2 counterpart. This means it is having a good range of combinations from both Type2 and Departmental. It must be offering mostly premium products but at the same time having some most poupular lower of Mid range products. Type1 is size \"High\", so it defeintely can be renovated to fit more products by renovations and give more options to the customers.\n",
    "\n",
    "- Food Market has lowest mean. It is more convienence store for quick and small purchases.\n",
    "\n",
    "- The important thing is that the distribution of the Product Weights across Product Type is same for all the 4 Stores. It shows that even though the stores are selling different sizes of products, yet, the % distrbution of products types across the stores is NOT changing significantly.\n",
    "\n",
    "- Other interesting observations are:\n",
    "    - On right side pie chart. Even though it is SUM of all Area in the dataset, it is indirectly recflective of overall sales per store (Because it is just a sum and more orders means the value goes higher). It is showing that Area is not really correlated with Weight. Because if it was correlated then Type1 has lower mean than Departemental, but Area is having value in Type2 and lower in DEpartmental. It is actually directly proportional only to the orders and not really the weight of the product.\n",
    "    - The orders are lower for Departmental stores (which had marginally higher sales than Type1 SuperMarket) compared to Type 1 SuperMarket and Type 2 SuperMarket. But it exceeded sales for Type 1 and it has higher % revenue per order when compared to Type 2 (Even though sales is lower). Again it shows the Departmental is focusing on higher Premium products probably with higher MRPs.\n",
    "    - The %s on the left pie and right pie are also not corrrelated. (For ex: Seafood, breakfast and Breads) Weight %s are all > 5% but the corresponding area sum are very less. Which means these products are given lesser display area (Allocation) and therefore the values are low. Also for ex: Frozen Foods, Meat, Household are some instances where the display areas are more than the Weights of the products. This also proves there is no direct correlation between the two.\n",
    "    - The left side pie show no. of order in brackets (), we can see the right side Area is directly proportional to this number of oeders as it is sum of areas of the items sold. But it is no way related to the Weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {
    "id": "9a4fdf46"
   },
   "source": [
    "#### **Understanding Product MRP**\n",
    "- We will do exactly similar analysis as we did about for Product Weight.\n",
    "- The intention will be too see if the hypothesis we made above that, Departmental Stores are seeling premium products with higher MRP is True or Not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2477b856",
    "outputId": "f22ee4e1-34fa-405e-c087-814868261a87"
   },
   "outputs": [],
   "source": [
    "psk = skart.groupby([styp,ptyp]).agg({pwt:'mean', pmrp:'mean', pid:'count'}).unstack(level=0)\n",
    "psk.columns = [y+(\" Weight\" if x == pwt else \" Pid\" if x == pid else \" MRP\") for x,y in psk.columns]\n",
    "\n",
    "\n",
    "ctyp = \"Dummy\"\n",
    "def autopct_format_factory(df_vals):\n",
    "\tdef autopct_format(pct, all_vals=df_vals):\n",
    "\t\t\t\tautopct_format.counter += 1\n",
    "\t\t\t\tidx = autopct_format.counter - 1\n",
    "\t\t\t\t# lis = list(skart[styp].unique())\n",
    "\t\t\t\tcolnm = ctyp+\" Pid\"\n",
    "\t\t\t\tabs_val = all_vals.iloc[idx][colnm]  # pick the column you want\n",
    "\t\t\t\treturn f\"{pct:.1f}%\\n({abs_val})\"\n",
    "\tautopct_format.counter = 0\n",
    "\treturn autopct_format\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(4,2,figsize=(30,50) )\n",
    "ax = ax.flatten()\n",
    "\n",
    "for n,i in enumerate(skart[styp].unique()):\n",
    "\tctyp = i\n",
    "\tax1 = ax[2*n]\n",
    "\tax2 = ax[2*n+1]\n",
    "\ty1c = i+\" MRP\"\n",
    "\ty2c = i+\" Weight\"\n",
    "\ty4c = i+\" Pid\"\n",
    "\tax1.pie(psk[y1c], labels=psk.index, autopct=autopct_format_factory(psk), textprops={'fontsize': 18}, radius=1.1)\n",
    "\tax1.set_title(y1c+\" AVG SUM = \"+str( round(psk[y1c].sum(),3))+\" (Orders= \"+str(psk[y4c].sum()) +\")\", fontsize=20, fontweight='bold')\n",
    "\tax2.pie(psk[y2c], labels=psk.index, autopct='%1.1f%%',   textprops={'fontsize': 18}, radius=1.1)\n",
    "\tax2.set_title(y2c+\" AVG SUM = \"+str(round( psk[y2c].sum(), 1) )+\":\\n MRP Price per 1 unit of Weight =\"+str(round ( psk[y1c].sum() / psk[y2c].sum(), 1) ), fontsize=20, fontweight='bold')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {
    "id": "d2fa37c0"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "1. The left hand side pie is having AVG MRP and right hand side is having AVG weight. The left side figure for AVG MRP is expected. AVG MRP is not directly influenced by number or orders. It is influenced by the value of each order.\n",
    "     - Even though highest sales are in Type2 SuperMarket its AVG MRP is less at 2277 as opposed to Deprtmental value at 2919.\n",
    "     - It shows lower value products are being sold by Type2 Store when compared to Departmental. But, no. of orders are high for the same (which is cause of higher sales).\n",
    "2. The same is not true (infact the oppsite) for Departmental Stores. It has lower number of orders and higher value for each order. It makes sense from previous analysis as Departmental stores are selling premium products of higher weight and hence higher order value.\n",
    "But, what is interesting is the right hand side pie chart.\n",
    "    - AVG Weight and Ratio of MRP to 1 unit of Weight is Shown in the second line of the title of the pie.\n",
    "    - It shows lowest value of 10.7 for Food Market. Departmental is at 12 units which is the highest and Type2 SM having highest sales is in between at 11.5. The Type1 SM is closer to Deprtmental store.\n",
    "    - This is because of the Location of stores. Departmental store is located in \"Tier 1\". High cost of living. Inspite of the bulk purchase discounts the effective price per unit of weight is comming to 12.\n",
    "    - On the other hand Food Market is \"Tier 3\". Low cost of living. Inspite of selling smaller units which usually must cost higher is lower than \"Tier 1\" cost.\n",
    "    - SuperMarket are in \"Tier 2\" cities they are therefore in the middle and \"Type1\" SM is higher than Type1 as it selles higher Weights (closer to Departmental)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {
    "id": "5641d246"
   },
   "source": [
    "#### **Understanding Product Allocation Area**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {
    "id": "8564676f"
   },
   "source": [
    "##### **Product Allocation area in Stores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e626774a",
    "outputId": "cc47a6c8-a388-4fef-f57d-5304807e66c4"
   },
   "outputs": [],
   "source": [
    "print ( \"paa area across stores \", skart.groupby(sid)[paa].sum(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "id": "8340d48e"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "The \"OUT004\" Store with highest sales and sum of Production allocation areas is highest. As the dataset given is unique on product Ids one would expect this Area to be close to 1 as it is ratio by defintion.\n",
    "This needs more analysis. Let us computes the Areas per store and Product types. Check the mean values against sales values across all store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {
    "id": "171a2864"
   },
   "source": [
    "##### **Understanding Product Allocation Area across Stores and product types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1b30323",
    "outputId": "a136703c-7b4c-4b59-ea48-5336e01d2048"
   },
   "outputs": [],
   "source": [
    "pskw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "44bc4436",
    "outputId": "bff1dc9f-b290-4e42-d70d-ee0d50ad13f1"
   },
   "outputs": [],
   "source": [
    "pskw = skart.groupby([styp,ptyp]).agg({target:'sum', paa:['mean','sum'], pid:'count'}).unstack(level=0)\n",
    "pskw.columns = [z+ y +(\" Sales\" if x == target else \" Pid\" if x == pid else \" Area\") for x,y,z in pskw.columns]\n",
    "# print( pskw.columns)\n",
    "ctyp = \"Dummy\"\n",
    "def autopct_format_factory(df_vals):\n",
    "\tdef autopct_format(pct, all_vals=df_vals):\n",
    "\t\t\t\tautopct_format.counter += 1\n",
    "\t\t\t\tidx = autopct_format.counter - 1\n",
    "\t\t\t\tcolnm = ctyp+\" Pid\"\n",
    "\t\t\t\tabs_val = all_vals.iloc[idx][colnm]  # pick the column you want\n",
    "\t\t\t\treturn f\"{pct:.2f}%\\n({abs_val})\"\n",
    "\tautopct_format.counter = 0\n",
    "\treturn autopct_format\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(4,2,figsize=(30,50) )\n",
    "ax = ax.flatten()\n",
    "\n",
    "for n,i in enumerate(skart[styp].unique()):\n",
    "\t\t\tctyp = i+\"count\"\n",
    "\t\t\tax1 = ax[2*n]\n",
    "\t\t\tax2 = ax[2*n+1]\n",
    "\t\t\ty1c = i+\"mean\"+\" Area\"\n",
    "\t\t\ty11c = i+\"sum\"+\" Area\"\n",
    "\t\t\ty2c = i+\"sum\"+\" Sales\"\n",
    "\t\t\ty3c = i+\"count\"+\" Pid\"\n",
    "\t\t\t# print( y1c,y11c,y2c,y3c)\n",
    "\t\t\tax1.pie(pskw[y1c], labels=pskw.index, autopct=autopct_format_factory(pskw), textprops={'fontsize': 14}, radius=1.1)\n",
    "\t\t\tax1.set_title(y1c+\" SUM = \"+str( round(pskw[y1c].sum(),3))+\" (Orders= \"+str(pskw[y3c].sum()) +\")\", fontsize=20, fontweight='bold')\n",
    "\t\t\tlabels = [f\"{idx}\\n({round(val, 2)}K)\" for idx, val in zip(pskw.index, pskw[y2c]/(1000*pskw[y11c]))]\n",
    "\t\t\tax2.pie(pskw[y2c], labels=labels, autopct='%1.2f%%',   textprops={'fontsize': 14}, radius=1.1)\n",
    "\t\t\tax2.set_title(y2c+\" SUM = \"+str(round(pskw[y2c].sum(),2))+\"\\n Average sales per Area = \"+str(round(pskw[y2c].sum()/(1000*pskw[y11c].sum()),2))+\"K\", fontsize=20, fontweight='bold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {
    "id": "3779e30a"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "General understanding of the pie charts\n",
    "    - The product Allocation Area are ratios of the product area to its other products within the same Product Type per store. It is not the ratio of the total Product area across all the products in all the product types. This is confirmed mean value which is almost 1 for all the stores in the left hand side of the pie charts.\n",
    "    - The pie charts in left side are created one per Store Type and each store type has 16 product types are shown in the pie chart. The orders are shown inside the pie chart with () and overall orders are shown in the title for the store.\n",
    "    - The %s inside the pie chart are the Area occupied %s for the product types averaged per product type.\n",
    "    - the right side pie chart are overall sales per store divided into product types in the pie chart %s.\n",
    "    - the labels of the pie chart have anotehr %s in (), these are showing %s of sales / Area %s (which are in left pie chart)\n",
    "\n",
    "Observations:\n",
    "- The Product Allocation Areas are very similar in all the 4 stores. This is confirmed by the pie chart Left hand side columns for Area. Each Store in % has almost same allocation when we take the Mean Value. The Min and Max are varying within a product type group. But overall %s of display area allocations are very similar with minor variations done by Store business owners.\n",
    "- The Sales is not corresponding to the Area. Espically with some products like 'Starchy Foods', 'Others', 'Breads, 'Breakast', 'seafood' are the  items in all stores which take more allocation area but are generating less revenue.\n",
    "- This could also mean that products in these take lesser area or just having lesser sales. This is also input to business to check if the allocation areas need optimization or an innnovative management non-display storage and display area storage. It could also mean the Store can reduce allocation Area and this might influence overall sales.\n",
    "\n",
    "-Ket take away. Area has an indirect impact on Sales. No direct correlation. By reducing the allocation areas for lesser sold products the display areas can be maximized for other products which sell better. This way Area influnces Sales but not directly.\n",
    "- Moreover, we see that Area can be +vely and -vely correlated with sales. \"Baking Goods\" for example has less Area and higher Sales. But Breads it is the opposite. This is indicative of the fact that this is not a direct correlation.\n",
    "\n",
    "- Product area allocation is determined and controlled by the store administration and the produt weights / produtMRP is agreed mutually between store administration and product venders. Therefore, one can have different weights and MRPs having same allocation area. Also, the same weights for lighter products can have lower or higher MRPs based on the produt type. Simmilarly smaller products can fit easily into smaller areas ans vice versa. These possibilities make the relationship between MRP,Area,Weight very complex. Hence it is not visually determine pattrens of correlations. The pattern are complex and deeply embedded into the product types and store types.\n",
    "\n",
    "**IMPORTANT**\n",
    "- **Based on the analysis MRP, AREA and Allocation, one can conclude that none of this have a well linear correlation with Sales. It is a complex relationship influenced by many factors and better handeld by the ensemble models. It is easy for visualization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {
    "id": "70e6f092"
   },
   "source": [
    "#### **Number of products sold in each store**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffe84d3d",
    "outputId": "16befcb9-ffe2-47ca-e4a7-ada80f77b4de"
   },
   "outputs": [],
   "source": [
    "print ( \"No of Products sold in each store \", skart.groupby(sid)[pid].nunique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {
    "id": "50bbd58b"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- From this data it is pretty clear as to why the \"OUT004\" is having high sales. It simply is giving a very product range.\n",
    "- There could be many factors contributing this but the stores businesses must look at how to maximize there allocation areas and increase there products range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {
    "id": "009d2ee0"
   },
   "source": [
    "#### **Are all the products are sold atleast by one or more stores ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f681791",
    "outputId": "5c71b8ba-0b4f-436f-964c-c5a8f77d90a1"
   },
   "outputs": [],
   "source": [
    "print ( \"Sum of all the products sold across stores \", skart.shape[0], sum( skart.groupby(sid)[pid].nunique().values))\n",
    "print ( \"Products sold in multiple stores. \", sum( skart.groupby(pid)[sid].count() > 1) )\n",
    "print ( \"Number of product types sold by each store \", skart.groupby(sid)[ptyp].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {
    "id": "ed55b9f4"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- All Stores are selling all the 16 product types.\n",
    "- The product IDs are unique and they do not overlap across the stores. It suggests that stores may be geographically in different areas where the vendors supplying the products are different and hence different product IDs. Standardized venodrs across stores may not be present or may not given unique IDs across stores. This is also something the business can look to standardize so that the products doing well in some cities can be offered to other cities.\n",
    "The first two chars of Product ID might be useful is studying any pattern of sales across these as an addtional categorical variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {
    "id": "9cda46a7"
   },
   "source": [
    "#### **Top five products types across all stores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fbd82ef",
    "outputId": "b6f591c1-f917-4ef7-da42-44237d2b2c1d"
   },
   "outputs": [],
   "source": [
    "print ( \"Top five selling products type \\n\", skart.groupby(ptyp)[target].sum().sort_values(ascending=False).head(5))\n",
    "print ( \"Top five selling products type per store \\n\", (skart.groupby([ptyp, styp], as_index=False)[target].sum()).sort_values([styp, target, ptyp], ascending=[True, False, True]).groupby([styp]).head(5) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {
    "id": "31726a13"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- THE TOP PRODUCT TYPES ACROSS ALL STORES ARE SAME. THE VOLUMES OF SALES VARY.\n",
    "- This suggests that people are comming to the stores for top 5 categories and they will experiement if new products are offered in these categories. The other stores must explore the products from \"OUT004\" and optimize there allocation area to also shelf them to increase the sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {
    "id": "4bc0c2ec"
   },
   "source": [
    "#### **Stores Top 10 Allocation Area per product type**\n",
    "- Check if the stores are allocating area correctly as per the sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "faacc62c",
    "outputId": "d70983df-f646-4981-9da9-45009b66535b"
   },
   "outputs": [],
   "source": [
    "store_paa = skart.groupby([styp, ptyp], as_index=False).agg ({paa:'sum'}).sort_values(by=[styp, paa], ascending=[False,False]).groupby([styp]).head(10).reset_index().loc[:,[styp,ptyp,paa]].pivot_table(index=styp, columns=[ptyp], values=paa, aggfunc='sum').sort_values(by='Supermarket Type2', axis=1, ascending=False)\n",
    "store_paa['type']='paa'\n",
    "\n",
    "store_sales = skart.groupby([styp, ptyp], as_index=False).agg ({target:'sum'}).sort_values(by=[styp, target], ascending=[False,False]).groupby([styp]).head(10).reset_index().loc[:,[styp,ptyp,target]].pivot_table(index=styp, columns=[ptyp], values=target, aggfunc='sum').sort_values(by='Supermarket Type2', axis=1, ascending=False)\n",
    "store_sales['type']='sales'\n",
    "\n",
    "store_paa_vs_sales = pd.concat( [store_sales.reset_index(), store_paa.reset_index()], axis=0, ignore_index=True )\n",
    "store_paa_vs_sales[styp] = ( (store_paa_vs_sales[styp]).astype(str) + \"__\" + store_paa_vs_sales['type'] ).astype('category')\n",
    "store_paa_vs_sales.reset_index(drop=True).sort_values(by=[styp, ], ascending=[False]).iloc[:,0:11]\n",
    "\n",
    "# store_paa_vs_sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {
    "id": "9e00019d"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- The Top 10 allocation Area Product Types are the same for all the stores. We computed only top 10 entries and Non Zero values in these entries indicate that the categories are same, only the sales % different.\n",
    "- The allocation area sizes are correctly adjusted as per sales for all the stores in most cases. There are a few analomies as we saw earlier for items like Seafood in some Stores which need inspection.\n",
    "- For ex: check \"Household\" and \"Frozen Foods\" items in \"Supermarket Type2\" and \"Food Mart\". In the SuperMarket sales for \"Frozen Foods\" is higher. In the \"Food Market\" sales for \"Household\" is higher. But Allocation area is adjusted correctly in both according to the sales.\n",
    "- It shows that Stores have adjusted there respective areas as per there sales. However considering the success of the \"OUT004\" there is need for oother stores to explore the other product types and better layout for maxmizing allocation area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {
    "id": "f95449d1"
   },
   "source": [
    "## **Feature Engineering - Part 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {
    "id": "a65c70f4"
   },
   "source": [
    "#### **Add Product ID first two hars as a seperate column**\n",
    "- In Model Building we will remove Product ID\n",
    "- But the first two chars of product are having a specific meaning as classify the producs differently\n",
    "- An anlysis of this also might be useful in identifying any patterns on sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "0696d835",
    "outputId": "5b2c5bc5-e3ef-4eba-d1a4-1f780bf7372d"
   },
   "outputs": [],
   "source": [
    "pidc2 = \"pid_c2\"\n",
    "skart[pidc2] = skart[pid].apply(lambda x: x[0:2])\n",
    "categories = categories + [pidc2]\n",
    "products[pidc2] = products[pid].apply(lambda x: x[0:2])\n",
    "labeled_barplot(skart, pidc2, perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {
    "id": "50e396ef"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- As expected the Food items are having highest order share compared to other two categories.\n",
    "- More analysis is needed if the %s of fooditems ar similar across stores or diferent. This will be done in Bi-variate sections. If analysis proves that they are similar then we drop Product type and use the PIDC2 in model building. Otherwise we drop PID_C2 and keep the product type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {
    "id": "59e0b2f7"
   },
   "source": [
    "#### **Convert all Object datatypes to Categorical types for Univariate analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {
    "id": "cd755ca1"
   },
   "source": [
    "##### **Prepare the ordinal order for each categorical variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {
    "id": "242ab1f8"
   },
   "outputs": [],
   "source": [
    "vcdf = pd.DataFrame ( [{ 'Var': x, 'Total': skart[x].nunique(), 'Category': yind, 'Count': yval } for x in categories for yind, yval in skart[x].value_counts().items() ] )\n",
    "psc_order = [\"No Sugar\", \"Low Sugar\", \"Regular\"]\n",
    "ptyp_order = vcdf.loc[(vcdf['Var']==ptyp), ['Category', 'Count']].sort_values(by='Count', ascending=True)['Category'].values.tolist()\n",
    "ssize_order = [\"Small\", \"Medium\", \"High\"]\n",
    "sloctype_order = [\"Tier 1\", \"Tier 2\", \"Tier 3\"]\n",
    "styp_order = vcdf.loc[(vcdf['Var']==styp), ['Category', 'Count']].sort_values(by='Count', ascending=True)['Category'].values.tolist()\n",
    "seyr_order = vcdf.loc[(vcdf['Var']==seyr), ['Category', 'Count']].sort_values(by='Count', ascending=True)['Category'].values.tolist()\n",
    "pidc2_order = vcdf.loc[(vcdf['Var']==pidc2), ['Category', 'Count']].sort_values(by='Count', ascending=True)['Category'].values.tolist()\n",
    "cat_order_map = {psc:psc_order, pidc2:pidc2_order, ptyp:ptyp_order, ssize:ssize_order, sloctype:sloctype_order, styp:styp_order, seyr:seyr_order}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {
    "id": "78ae0aa9"
   },
   "source": [
    "##### **Convert to Categorical and use the ordinal order to set into cat.codes directly**\n",
    "- This way no special ordinal encoding is required.\n",
    "- All categorical has a internal ordinal order except the product type.\n",
    "- There are 16 product types and all stores are selling all product types.\n",
    "- As we saw earlier the top 10 categories of the product types are same and only the sales are differing.\n",
    "- Therefore even for the product type we use Label encoding by taking overall sales as the ordinal order.\n",
    "- This will avoid multi dimensional one hot encoding complexity and curse of dimensionality which is not really necessary in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e3c3d26",
    "outputId": "54feac0b-e7a0-441d-b68e-5d8523d62e62"
   },
   "outputs": [],
   "source": [
    "# Even though Established year is Numeric it is better handled as Categorical. So changing it.\n",
    "print (categories)\n",
    "for ft in categories:\n",
    "    # Here pass\n",
    "    skart[ft] = pd.Categorical(skart[ft], categories=cat_order_map.get(ft), ordered=True)\n",
    "\n",
    "# [print ( dict(zip( skart[x].cat.categories, range(len(skart[x].cat.categories)))), skart[x].cat.codes.tolist()) for x in categories]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {
    "id": "e8e9a22b"
   },
   "source": [
    "## **Univariate Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {
    "id": "a73bdfaf"
   },
   "source": [
    "### **Plot (count plot) all categorical variables with sub categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ea16ad12",
    "outputId": "e1dffa78-cd40-4333-d25b-dc1552d594c5"
   },
   "outputs": [],
   "source": [
    "# Use labelling and plot a countplot which looks like a barplot with avalue on the top.\n",
    "cats = categories+[target]\n",
    "skart_cat = skart.loc[:, cats]\n",
    "\n",
    "# Do it for all the categorical variables.\n",
    "for ft in categories:\n",
    "    labeled_barplot(skart_cat, ft, True)\n",
    "\n",
    "# Also count stores and number of products in each store.\n",
    "labeled_barplot(skart, sid, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {
    "id": "14b50637"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "\n",
    "**Product_Sugar_Content**\n",
    "- \"Low Sugar\" products categories are selling the most compared to \"Regular\" and \"No Sugar\".\n",
    "\n",
    "**Product_Type**\n",
    "- \"Fruits and Vegetables\" are the highest sellers followed by \"Snacks\" and then by \"Frozen Foods\" and \"Dairy\". \"Seafood\" and \"Starchy Foods\" are at the bottom of the sales.\n",
    "\n",
    "**Store_Size**\n",
    "- \"Medium\" Size Store are recording highest sales compared to \"Small\" and \"High\".\n",
    "\n",
    "**Store_Location_City_Type**\n",
    "- \"Tier 2\" location stores are recording the highest sales compared to \"Tier 1\" and \"Tier 3\"\n",
    "\n",
    "**Store_Type**\n",
    "- Out of the Four stores \"OUT004\" store which is \"Tier 2\" and \"Medium\" size has recorded the highest sales. This is also the primary influencer for the high sales in Tier 2\" and \"Medium\" size classes.\n",
    "\n",
    "**Store_Establishment_Year**\n",
    "- The latest establish Store in 2009 has recorded high sales giving an impression that customers are preferring the modern ambience and outward look of the stores.\n",
    "\n",
    "**Product_ID (first two characters)**\n",
    "- This variable is derived from the Product ID. It has three values FD (Food items), DR (Drinks) and NC (Non Consumables). This addtional variable was created for analyzing any patterns of deviation across the different stores.\n",
    "- We can see in the above Count plot that FD items are highest selling, followed by Non Consumables and finally drinks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {
    "id": "6f24aca0"
   },
   "source": [
    "### **Plot (KDE/Histogram and Box) for numeric vairables**\n",
    "- For numeric data fields create an addtional (equivalent) categorical field by binning into a max 6 bins as per quantiles and outilers\n",
    "- Binning will allow additional insights on the numeric data as it can be cross tabbed with categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "587385fc",
    "outputId": "ff10151a-4d32-47b0-d961-d3a7d4638677"
   },
   "outputs": [],
   "source": [
    "nums = numeric_vars\n",
    "print(nums)\n",
    "\n",
    "# Data is pd.Series. It will compute Quantiles, IQR and generate bins according to quantiles including outliers\n",
    "def gen_bins (data):\n",
    "\n",
    "    # Calculate quartiles and IQR\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Outlier thresholds\n",
    "    lower_outlier = Q1 - 1.5 * IQR\n",
    "    upper_outlier = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Define bin edges\n",
    "    bin_edges = [\n",
    "        -np.inf,                 # extreme lower outlier start\n",
    "        lower_outlier,           # lower outlier end\n",
    "        Q1,                      # first quartile\n",
    "        data.median(),           # median\n",
    "        Q3,                      # third quartile\n",
    "        upper_outlier,           # upper outlier start\n",
    "        np.inf                   # extreme upper outlier end\n",
    "    ]\n",
    "\n",
    "    # Define bin labels\n",
    "    bin_labels = [\n",
    "        \"Lower Outlier\",         # < lower_outlier\n",
    "        \"Q1 Range\",              # between lower_outlier and Q1\n",
    "        \"Q2 Range\",              # between Q1 and median\n",
    "        \"Q3 Range\",              # between median and Q3\n",
    "        \"Q4 Range\",              # between Q3 and upper_outlier\n",
    "        \"Upper Outlier\"          # > upper_outlier\n",
    "    ]\n",
    "    return bin_edges, bin_labels\n",
    "\n",
    "target_bin = target+\"_bin\"\n",
    "paa_bin = paa+\"_bin\"\n",
    "pmrp_bin = pmrp+\"_bin\"\n",
    "pwt_bin = pwt+\"_bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9a94c373",
    "outputId": "d68fea49-89ca-4a2e-e2cf-1e8a73b88202"
   },
   "outputs": [],
   "source": [
    "skart_num = skart.loc[:, nums]\n",
    "for ft in nums:\n",
    "    bin_edges, bin_labels = gen_bins(skart_num[ft])\n",
    "    skart_num[ft+\"_bin\"] = pd.cut(skart_num[ft], bins=bin_edges, labels=bin_labels)\n",
    "    histogram_boxplot(skart, ft, kde=True, bins=20)\n",
    "# Binned numeric data is also useful for categorical analysis. Therefore duplicate into Start_cat\n",
    "length = len(nums)\n",
    "columns = skart_cat.columns.append(skart_num.columns[length:length*2])\n",
    "skart_cat = pd.concat( [skart_cat, skart_num.iloc[:,length:2*length]], axis=1, ignore_index=True)\n",
    "skart_cat.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {
    "id": "18af82f8"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "\n",
    "**Product_Weight**\n",
    "- Nice Symetric data forming a bell curve.\n",
    "- There are Outliers (shown in the box plot) on either side of mean.\n",
    "- this is expected because there a lot of ranges of product Weights. they are all well controlled into standards size of oz and lbs.\n",
    "- So, in the supermarket product inventory this is expected to have this kind of symetric graph for food products and also outliers here is not a problem.\n",
    "\n",
    "**Product_Allocated_Area**\n",
    "- Right Skewed data. Mean is being pulled to right due to Outliers on the right hand side.\n",
    "- This shows that there are some products which are taking higher allocation areas and a lot of smaller products taking lower allocation Areas.\n",
    "- this Right skewness is also expected because there will be many products in smaller sizes which will not demand a lot of allocation area.\n",
    "- On the other hands, there will be some products which are bigger packages in Frozen foods etc. of higher quantities which take larger areas and are fewer in numbers.\n",
    "- The right side outliers here also can be safely ignored. the key aspect to check here is \" Are the sales proportional to display allocation areas that are allocated ?\"\n",
    "\n",
    "**Product_MRP**\n",
    "- Again like the Product_Weight this data is very Symetric and has outliers on either side of the mean.\n",
    "- This is also expected as the MRP will have a lot of ranges for all the 8763 product ids in the dataset.\n",
    "- It will defintely go beyond the permissiable lower and upper bounds calculate using the IQR.\n",
    "- The important things to check here is category ranges or MRPs which are most seeling giving an indication of how much users are usually spending in each of the product categories.\n",
    "\n",
    "**Product_Store_Sales_Total**\n",
    "- This is also a nice symeetirc grpah. It shows that sales are symetric all combination of products are being purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {
    "id": "1df1eb4b"
   },
   "source": [
    "## **Bivariate/Multivariate Analysis with target variable**\n",
    "- Use skart_num dataframe for Numeric data fields. It has bins created for each numeric field. With bins it is possible the Numeric field as Category vairables also.  \n",
    "- Use skart_cat dataframe for studying Categorical vairables influence target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {
    "id": "202b7d7d"
   },
   "source": [
    "### **Need for creating a transformed dataset for Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {
    "id": "c6b84673"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- As we saw in the Basic analysis the relationships of variables like Product_Weight, Product_Allocation_Area and Product_MRP are very dependent on the Product_Type\n",
    "- The Store size, Location, Even establishment year are influencing the overall products prices, weights and closely related to Store Type.\n",
    "- A general analysis of features will loose many patterns. The features have to organized as product and stores ids and there means, sums, max and min must be taken for a better analysis.\n",
    "- Therefore lets create a transformed dataset that represents the relationships of features more closely.\n",
    "- Also, As the numeric variables have different scales, like Product Area is 0 to 1, Target variable is very high numeric values, MRP and Weight have intermmediate values it is diffiuclt to analyze the correlations without Scaling.\n",
    "- This transformed dataset must scale the numeric variables. The main pupose of this dataset will be idenity patterns and not actually to find actual values or metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5fe043d",
    "outputId": "5ce4538c-082e-4b14-b59c-89919fbd8e8b"
   },
   "outputs": [],
   "source": [
    "skart_AW = skart.groupby([styp, ptyp]).agg( {paa:['max', 'min', 'mean'], target:['sum','count','mean'], pmrp:['mean','max','min'], pwt:['mean','max','min'] })\n",
    "skart_AW = skart_AW.stack(level=[0,1]).reset_index()\n",
    "skart_AW.columns = [styp, ptyp, 'feature', 'aggfunc', 'value']\n",
    "\n",
    "# As the Sales is very high number and Product Weights and product Allocation Area are small numbers\n",
    "# Visualization is difficuly. Hence lets scale the values in 0 to 1 range.\n",
    "# the Plot will not show actual numbers but we are interested in relative increase\n",
    "# So we ignore actual numbers and check relative increase.\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# Scale the DataFrame\n",
    "mask_target = ( (skart_AW['value'] >=1 ) & (skart_AW['feature'] == target) )\n",
    "skart_AW.loc[mask_target, 'value'] = scaler.fit_transform(skart_AW.loc[mask_target, ['value']])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "mask_mrp = ( (skart_AW['value'] > 1) & (skart_AW['feature'] == pmrp) )\n",
    "skart_AW.loc[mask_mrp, 'value'] = scaler.fit_transform(skart_AW.loc[mask_mrp, ['value']])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "mask_wt = ( (skart_AW['value'] > 1) & (skart_AW['feature'] == pwt) )\n",
    "skart_AW.loc[mask_wt, 'value'] = scaler.fit_transform(skart_AW.loc[mask_wt, ['value']])\n",
    "\n",
    "\n",
    "# For use which require the long format one can use the below dataset.\n",
    "skart_long = skart_AW.pivot(index=[ptyp,styp], columns=['feature', 'aggfunc'], values='value')\n",
    "skart_long.columns = [x+\"_\"+y for x,y in skart_long.columns]\n",
    "skart_long.reset_index(inplace=True)\n",
    "\n",
    "# Add the key metrics which are used in this dataset.\n",
    "skart_long['mrp_2_weight'] = skart_long[pmrp+\"_mean\"]/skart_long[pwt+\"_mean\"]\n",
    "skart_long['area_2_sales'] = skart_long[target+\"_mean\"]/skart_long[paa+\"_mean\"]\n",
    "\n",
    "#Add product types to skart_long\n",
    "skart_long = skart_long.merge(products.loc[:, [ptyp, pidc2]].drop_duplicates(), on=ptyp, how='left')\n",
    "#Add Store attributes to skart_long\n",
    "skart_long = skart_long.merge(stores[[styp, ssize, sloctype, seyr]], how='left', on=styp)\n",
    "skart_long[pidc2] = pd.Categorical(skart_long[pidc2])\n",
    "skart_long[sloctype]= pd.Categorical(skart_long[sloctype])\n",
    "skart_long[seyr] = pd.Categorical(skart_long[seyr])\n",
    "skart_long.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {
    "id": "0a399e0d"
   },
   "source": [
    "### **Heat Map Analysis for Numeric variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89b12d20",
    "outputId": "78424f2e-05ac-4859-fc12-83ae1ec175ff"
   },
   "outputs": [],
   "source": [
    "# As this is packed visual, to save space we use smaller featue names\n",
    "vars = ['P_Wgt', 'P_Area', 'P_MRP', 'Sales']\n",
    "print( \"Using 'P_Wgt' for 'Product_Weight', \\n'P_Area' for 'Product_Allocated_Area', 'P_MRP' for 'Product_MRP', 'P_Type' for 'Product_Type','S_Type' for 'Store_Type' and 'Sales' for the target\")\n",
    "skart_hmaps = skart.loc[:, [ptyp,styp,pwt,paa,pmrp,target]]\n",
    "skart_hmaps.columns = ['P_Type', 'S_Type', 'P_Wgt', 'P_Area', 'P_MRP', 'Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6d1d89c7",
    "outputId": "82e5f9d9-1fa1-4652-c0a8-2d192081439b"
   },
   "outputs": [],
   "source": [
    "FHMap(skart_hmaps, 'P_Type', 'S_Type', vars, ht=None, asp=None, tit=True, fnt=8, annot_fnt=10, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {
    "id": "dd6940eb"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "\n",
    "- The results of correlations in the overall heatmap (First heatmap above outside grid) show only the following correlations\n",
    "\n",
    "    - Correlation shown is between Product_weight and Product_Store_Sales_Total(target variable) as 0.7. This is not correct reflection of the reality. It is an average giving a wrong impression of the data. Therefore for better analysis the individual heatmaps classified as under unique combinations of Product_Type and Store_Type must be analyzed. This is shown above in 64 (16 product types X 4 Store Types) smaller heatmaps.  \n",
    "\n",
    "\t- There is also a strong correlation shown between RMP and Sales of 0.8. this is also not a correct reflection of the reality. The reality is sales attributed to SuperMarkets are correlated properly with sales. But Food MArket and Departmental weakly correlated with MRP. But as the orders number is dominated by the supermarkets this correlation showing up.  \n",
    "\n",
    "\n",
    "- Lets do some deeper analysis on this smaller heatmap grid computed one per product and Store combinations:\n",
    "\n",
    "**Product Area**\n",
    "- Check cell (1,2) = Seafood in Departmental Store in the Grid. It is the first row second cell in 16X4 heatmaps\n",
    "\n",
    "It shows that Area is strongly -vely correlated with sales. It means  \n",
    "- (1) \"Seafood\" is taking more space but effective revenue generated (sales) is low compared to other food items which occupy similar area.\n",
    "- (2) \"Seafood\" is 0 or +vely correlated in \"Food Market\" and \"SuperMaket Type1\". This is because the Allocation_Area (See the section \"Understanding Product Allocation Area\" above which shows the Pie chart) % is reduced by these stores. This is reducing correlation to the sales.\n",
    "- (3) For Type2 Supermaket the correlation is -ve but small. This is because the Area of products and thereby the Weight is smaller. Type2 store is not selling Premium Products. it is selling smaller sizes and therefore smaller and Area of allocation must have been allocated by stores.  \n",
    "\n",
    "**In Summary**\n",
    "- \"Seafood\" is occupying more Area in stores and not giving enough corresponding sales. This is action for corresponding Store business to check all -ve correlations with sales in this Heatmap\n",
    "- Check cells in first Column = food Mart for Starchy Foods, Breads, Cannned, Frozen Food. They are all falling under the same categories where Areas and MRPs have -ve correlations with sales. The Business of Food Mart must see if they can reduced the allocated Areas and workout even more smaller size packages for these items.\n",
    "- Check cells for \"Others\" row no 4 in the above diagram. It has positive correlation only in Departmental Store. All other stores have -ve correlation and 0 for type1 SM. This means \"Others\" are sold as premium products and Departmental store has decided to move away from general allocation of area guidelines to custom guidelines for this. We can confirm this observation by looking at pie chart in “Understanding Product Allocation Area” section above. The % is more than 7% in Departmental store compared to other stores which less or equal to 6%. Type1 SM as observed earlier is selling a mix or premium products and lower MRPs. It is little closer to Departmental and hence it has 0. But Type2 SM is loosing on Area with this. Food Market also is losing but less than Type2 SM.\n",
    "\n",
    "**Product_Weight**\n",
    "- It is well managed in Type2 SM and Type1 SM. The Weights for all products are +vely correlated with the sales. It shows that the products sizes (like 0.5 lb or 10oz ) can be available in many sizes but the right sizes and quantity (like 5 pack or 10 pack) etc. must be according to what the customers typically need. These stores (SuperMarkets) are managing it well. While Type2 SuperMaket has long ranges of size mid and lower category. Type1 SM has higher range of sizes and very few selected lower sizes(Refer to the Scatter plotting \"Influence of Product_weight\" section below)\n",
    "\n",
    "**Product MRP**\n",
    "- It shows strong correlations with supermarkets Type1 and Type1. Low correlations with Food Market and Departmental. This is because it depends heavily on following factors\n",
    "  - Number of orders NOT the sales values. The Supermarkets have higher number of orders. Departmental has higher sales that Type1 Super Market but less orders. Food Market also has less number of orders\n",
    "  - Another important factor is the range of MRPs. Refer to the graph below \"Transformed dataset analysis\". It shows that MRP range of the Supermarkets is almost same (Scaled value range 0.35 to 0.70). The range for Departmental is much higher and Food Market is much lower. In these MRP ranges of Food Market and/or Departmental (Premium products) it is more likely for higher MRP values having lower sales and vice versa. But Super Markets being daily consumption items it is more likely to balanced MRP to price correlations\n",
    "  - However, this reqires proper selection and Type1 Supermarket is doing a very good job at it. Type2 supermarket must learn to do a better job by adopting the MRP and products patters of type1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {
    "id": "c6c91fa1"
   },
   "source": [
    "### **Pair plot for Numeric data**\n",
    "- A traditional pair plot will be useful when plotted using hue of Product_Store_Sales_Total_bin.\n",
    "- We have defined bins according to Outliers, Quartiles and IQRs.\n",
    "- Additonally Lets plot a custom relationship of the variables relationship with Target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {
    "id": "78af30a4"
   },
   "source": [
    "#### **Original dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "39e9a6c7",
    "outputId": "209c47b7-962f-49ef-886f-0e1f02a4c74a"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=skart, hue=styp, diag_kind='kde', markers=[\"o\", \"s\", \"D\"])\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {
    "id": "6cbf8ce6"
   },
   "source": [
    "##### **OBSERVATIONS - Original dataset**\n",
    "\n",
    "**Product_Weight**\n",
    "- Departmental Stores and Type1 SuperMarkets are selling higher product MRPs ( Closer to Whole sale ). Departemental stores is soley focused on Higher Weights and MRPs (Premium products most likely with premium pacaking adding to weights).\n",
    "- The Type1 is also focused on Premium products. But it has equal focused on selectd products in the lower MRP range. The Green dots have a distinct pattern. They are higher range, a gap exists and then lower range. It seems to be a very well planned strategy.\n",
    "- The type1 store is established is 1987 and one will expect that it has matured in its strategy or which MRPs are best seeling and optimized the product ranges to maximize sales.\n",
    "- Type2 Supermarket is selling lower to Medium ranges of Product pacakges (Hence Average Weight)\n",
    "- Food Market is popular for low range of Weights. It sells \"Loose\" products low price and typically small ot 1 piece quantity.\n",
    "- Product Weight is showing a liner correlation with Target a variable. The tilt (angle)\n",
    "- the KDE graph show high for Type 2 refelcting high sales. For Type Green color KDE it shows two distinct clusters for two clusters of MRPs that it is targetting. The Food MArket and Depratemental KDE almost show mutualally exlcusive markets targetted by the two Stores.\n",
    "- overall it has +ve correaltion on the sales. But has many anamolies that are complex to visualize.\n",
    "\n",
    "**Product_Allocated_Area**\n",
    "- Patterns is similar for the 4 Stores with Product Weights. There is no real liner correlation. It is solely the discretion or Stores and standard guideline for all stores. There is no relation like higher  are for higher MRPs or higher weights.\n",
    "- The KDE shows right skewed. It means there are choosen set (typically highest sales) of products by the store which are alwasy in the higher display area and a long tail of other products with lower sales with a lesser distributed display area.\n",
    "- It is not showing any steep tilt. It is having a mixture of flat and +ve correlations.\n",
    "- the interesting aspect for business are some strong -ve correlations for selected product and store combinations like Departmental / Store influence the sales -vely. The business can look at these and correct the display areas allocation for these items.   \n",
    "- Overall it is difficult to see any direct correlation target. Wherever we see some correlation also it is very weak and or a -ve correlation.\n",
    "\n",
    "**Product_MRP**\n",
    "- It is roughly similar to the Weights bu not fully.  This is because of variation of prices due to cost of living. The angles are not so steep. Typically as Type1 and Departmental Stores are selling in bulking we should see that effective MRPs come down but that did not happen as the Departmental store is in \"Tier 1\" and \"Type 1\" is in \"Tier 2\". However considering that \"Tier 1 \" city it is good tradeoff to buy in whole sale and then MRP diference to \"Tier 3\" is very comparable.\n",
    "- Overall it is +vely correlated. But it has a few analmolies also like Areas. Therefore not so linearly correalted as the weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {
    "id": "d98a6181"
   },
   "source": [
    "#### **Use Transformed dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0048cf40",
    "outputId": "0f849f1e-70fe-4568-c64a-18b50410d9f4"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=skart_long.loc[:, [ptyp, styp, paa+\"_mean\", pmrp+\"_mean\", pwt+\"_mean\", target+\"_sum\"]], hue=styp, diag_kind='kde', markers=[\"o\", \"s\", \"D\"])\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {
    "id": "74d0965d"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- This transformed dataset uses the mean values instead of actuals. The means are aggegated on unique Product type and Store Type combinations.\n",
    "- This will show overall aggregated behaviour for all product and Stores. Individual behaviours are different as we saw in the heatmap.\n",
    "\n",
    "- KDE graphs\n",
    "  - MRP and Weights KDE Graphs shows distinctly that the products shelved by these 4 types of stores are mutually exclusive. It is also confirmed by the fact that in the dataset the PId are not overlapping with Sids.\n",
    "  - The Area KDE simplyb refelct the order numbers. Higher the order bigger the bell curve. No correlation with sales.\n",
    "\n",
    "Correlations of Numeric variables. This graph more accurately depicts the overall relationships.\n",
    "   - MRP and Area are NOT correlated. The colors are mixed in all ranges.  \n",
    "   - Correlation with Target variables\n",
    "      - MRP we see that the stores are focusing on different MRPs. Food Market is focused on products with lower MRP/Lower weights (more like Single piece). Departmental is focused on higer MRPs and Higher weights (whole sale, like 50 pack etc). Type1 SMarket is closer to Departmental. Type2 is is focused on Modile and Upper middle range MRPs and hence similar weights.\n",
    "      - So MRP and Weights are positively correlated with Sales. But, there are anamolies, we see in some scatter points where MRPs are higher and sales are lower, vice versa and same for Weights. This is simply giving an insight on the nature of the products and not any actionable recomendation for the businesses.\n",
    "      - Area is different because Store business get to choose display area allocation. For MRP and Weights they have choosen focused limits and therefore they are fixed per store. But Areas are choosen by Stores and hence it is mixed bag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {
    "id": "cccf6d01"
   },
   "source": [
    "### **Transformed dataset analysis of numeric variables**\n",
    "- (Product_Weight, Product_MRP and Product_Allocation_Area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cc5251b4",
    "outputId": "5e01c647-0c0b-42fa-b6a5-26d689c4edb2"
   },
   "outputs": [],
   "source": [
    "FBar(skart_AW.sort_values(by='feature'), ptyp, styp,'aggfunc','feature', 'value', ht=5, asp=0.8, fnt=12, tit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {
    "id": "87de5896"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "\n",
    "**IMPORTANT While using this Transformed dataset the values are not important. The values are scaled. The important thing to take note of is relative correlation patterns**\n",
    "\n",
    "**Product_Weight**\n",
    "\n",
    "- It is same pattern we can observe thruout the graph above. This Transformed dataset is showing the pattern very distinctly. Check the mean/Max values. The Mean value is smallest for Food Market (Selling single items, convienence Store), Highest for Departmental (Whole Sale) followed by Type1 (Vintage 1987 Store which is seeling close to Whole sale) and then followed by Type2 which selling lower to Average sizes for products.\n",
    "- This pattern is very consistent acros all the product types.\n",
    "\n",
    "**Product_Allocated_Area**\n",
    "\n",
    "It is showing very good data for this field which will help the business to analyze there sales Vs allocated Area. Some examples:\n",
    "- Seafood: Check the Product_Store_sales_Total and Product_allocated_Area fields. Departmental Store and Type2 SuperMarket we saw earlier had -ve correlation for sales and Seafood Area in the heatmap. Type1 had +ve correlation. Food MArket had a 0 correlation.\n",
    "- This graph shows clearly why this is happening. Departmental Store has allocated 0.20 (higher) Area and Sales is 0.02 whereas Type1 super market allocation 0.12 Area and Sales is same. Hence there is a difference in the correlation. Type2 on the other hand as sales of 0.05 but has a higher area 0.23 and therefore still it has 0.1 negative correlations.\n",
    "- This data is showing this patterns for many other fields for business to take a look and adjust there Allocated_Area in proportion to the Sales. food Mart for Starchy Foods, Breads, Cannned, Frozen Food are other examples. The Business can look at the heatmap -ve correlations and come to this chart to look at the misalligned Area Allocations\n",
    "\n",
    "**Product_MRP**\n",
    "- This pattern is also beautifully depicted by the Transformed dataset. Check the MAX value in a the cells of the Grid. The Departmental store is selling Premium and couple by the fact that it is Tier 1 city. This is influencing the MRP to the highest. This is followed by Type1 SMarket which is in Tier 2 but selling mix of premiun and lesser MRPs (high Weights). Then come Type1 which is selling Low and Average sizes and is Tier 2 and finally Tier 3 and small sizes in Food MArket.\n",
    "- this pattern is clearly visiible in all cells of the Graphs confirming the assumptions made during Heatmap and Pariplot analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {
    "id": "6771a300"
   },
   "source": [
    "### **Influence of **Product_Id(first two chars)** on **Product_Store_Sales_Total****\n",
    "- FD stands for Food items\n",
    "- DR stands for Drink items\n",
    "- NC stands for Non Consumables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {
    "id": "0e50aea3"
   },
   "source": [
    "#### **Percentage variation across Stores for these items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "id": "327b0af1",
    "outputId": "45fb1b65-bac0-41a0-aff7-2b360f5c39f8"
   },
   "outputs": [],
   "source": [
    "skart_pidc2 = skart.groupby([styp, pidc2]).agg({target:'sum'}).unstack(level=0)\n",
    "fig, ax = plt.subplots(2,2,figsize=(12,10))\n",
    "# ax.reshape(-1,1)\n",
    "print(styp_order)\n",
    "skart_pidc2.columns = [ x for y,x in skart_pidc2.columns]\n",
    "# skart_pidc2.columns\n",
    "for i,st in enumerate(styp_order):\n",
    "    axi = ax[i//2, i%2]\n",
    "    axi.set_title(st)\n",
    "    # df = skart_pidc2.loc[skart_pidc2[styp]==st]\n",
    "    axi.pie(skart_pidc2[st], labels=skart_pidc2.index, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {
    "id": "d6be9a7e"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Very similar % sales w.r.t to to these three product Types. The variation is less than 3% to 4%. This means that all the stores pretty much same products types are getting purchased and only the volume of sales is different.\n",
    "- This also suggests our earlier theory that number of products if increased it may boost sales.\n",
    "- FD = Food items. There are the majority sales\n",
    "- DR = Drinks. there are the least in all the stores.\n",
    "- NC = Non Consumables. Sales is greater than drinks and by far less than Food items. Even for this the variation in % sales is less than 3%.\n",
    "\n",
    "- **Key take away is that % sales in all stores is almost similar across product types. The deviation is only about 3% to 4%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {
    "id": "e308efc6"
   },
   "source": [
    "#### **Check actual percentage variations for food items**\n",
    "- If the variations of Product Types are not high then in Model building we drop Product Type and replace with pid_c2\n",
    "- If there are vairations which are getting lost by \"Averaging\" out at the FD category of pid_c2 then we can drop pid_c2 as redundant in Model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "06cbaf5f",
    "outputId": "313f6990-9d13-4575-d7d9-7b30794eb2fe"
   },
   "outputs": [],
   "source": [
    "cross_tab(skart_cat.loc[skart_cat[pidc2]=='FD'], ptyp, styp, nrows=2, ncols=1, figsize=(15,10), stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {
    "id": "a27673a6"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- The first crosstab is expected as we know that Type2 has higher sales and followed by departmental and Type1 stores. The least being Food Market. Nothing new here.\n",
    "- The seocnd crosstab shows the normalized values. It is showing that there are % variations of the product types in each store even though they are not of large scale. For example\n",
    "    - Breakfast is low in Type1. Meat and Starchy Food is higher Type1.\n",
    "    - Dairy and Starchy Foods are higher in Departmental Stores.\n",
    "    - Breakfast is high in Food Mart and Type2.\n",
    "- If we drop the Prduct Type and use pid_c2 then we loss these variations per store. So it is better to keep them and the ensemble/Tree models do a better job of anlayzing these values and finetuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {
    "id": "0d1cf463"
   },
   "source": [
    "### **Influence of Product_MRP on Product_Store_Sales_Total**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {
    "id": "fb857f2d"
   },
   "source": [
    "#### **Overall influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "09d19a52",
    "outputId": "84c5268b-9566-468e-9292-f93c12472679"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=skart_long, x=pmrp+\"_mean\", y=target+\"_sum\", hue=styp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {
    "id": "cebc3591"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Same as what we saw in the pairplot. +ve correlatiion between sales (target) and MRP exists but is weak one.  \n",
    "- Lower MRPs dominated by Food Market, Higher by Departmental and followed by Type1 and then Type2 in middle.\n",
    "- Type1 has intelligent mix of a few middle MRPs and mostly focused on higher MRPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {
    "id": "ab45c8cb"
   },
   "source": [
    "#### **Influence on Stores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "28a8216e",
    "outputId": "af384846-2461-4823-f4a3-67f5433d0c7f"
   },
   "outputs": [],
   "source": [
    "cross_tab(skart_cat, pmrp_bin, styp, nrows=2, ncols=1, figsize=(15,10), stacked=True, margins=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {
    "id": "08c6c242"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "This plot shows clearly MRPs and Stores relationships.\n",
    "- Food Mart is focusing on the lower MRPs.\n",
    "- Depratmental is focusing on the highest MRPs.\n",
    "- The Type1 is doing a smart trade-off. While focusing on higher MRPs it has selected low MRPs which it is selling.\n",
    "- Type2 is very active lower, Medium and upper Medium range. It is not focusing on higher MRP ranges (not into Whole sale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {
    "id": "c40502bb"
   },
   "source": [
    "#### **Product and Store wise influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "b2b13dda",
    "outputId": "cc7cc3ef-882b-4da3-d1b7-9175dbe90802"
   },
   "outputs": [],
   "source": [
    "FScatter(skart, ptyp, styp, psc, pmrp, target,ht=3, asp=1, fnt=7, tit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {
    "id": "859e15c4"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "This detailed Facet-Scatter plot on Product Type and Store Type with Product_Sugar_level hue is also giving some good insights.\n",
    "- First, it confirms the MRPs being higher on Departmental and Type1 in comparision to Type2 and least being Food Market. We can see the points are on the higher side of the graphs for Departmental and Type1 and lowest side for Food Market.\n",
    "- The interesting thing about this graph is shows for Type1 super market how it is not totally isolating itself from the lower MRP segement. There is distinct pattern in all product types where Type1 is actually selecting a few MRPs from the lower segement. Even though a large part of its presence is on higher MRPs and selected lower MRPs are targetted.\n",
    "- The different color for \"Health and Hygene, Others and Household shows they are non Consumables. This also eliminates the need of pid_c2 in the final Model building. It will be redundant.\n",
    "- The graph clearly shows that \"Low Sugar\" products are in demand. Majority of sales has happened in this category. Regular is also there but very less in relative percentages.\n",
    "- It also shows Type2 is focusing on a very narrow range of MRPs which will correspond closely to Weights of the pproducts and totally does not want to enter the lowest and highest segements of MRP spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {
    "id": "6afd3ddf"
   },
   "source": [
    "### **Influence of Product_Type on Product_Store_Sales_Total**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {
    "id": "51d32966"
   },
   "source": [
    "#### **Overall influence - Total sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "777c9c80",
    "outputId": "34fcc567-dd16-4c6a-9f50-ce2dea9354c6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,15))\n",
    "df = skart_AW.loc[((skart_AW['feature']==target) & (skart_AW['aggfunc']=='sum')) , [ptyp, styp, 'value']]\n",
    "sns.barplot(data=df, x='value', y=ptyp, hue=styp, orient='h', order=df.sort_values(by='value', ascending=False)[ptyp])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {
    "id": "a234d1c7"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- This is an expected graph. It uses overall sales and distributes it over product types and then sub sets with Store types.\n",
    "- As we know the highest sales is for \"Fruits and Vegetables' and Store Type2 Supermarket.\n",
    "- The Departmental is next closely followed by Type1. The graphs show there are a few product types ( Baking Goods, Hard Drinks, Soft Drinks and Household) where the sales of the Type1 exceeds Departemental. Even though in overall sales Departmental exceeds Type1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {
    "id": "16b4a4db"
   },
   "source": [
    "#### **Overall influence - Total orders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "115f5927",
    "outputId": "a7abcb4d-b32e-42ef-890a-61ef5fc7d579"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,15))\n",
    "df = skart_AW.loc[((skart_AW['feature']==target) & (skart_AW['aggfunc']=='count')) , [ptyp, styp, 'value']]\n",
    "sns.barplot(data=df, x='value', y=ptyp, hue=styp, orient='h', order=df.sort_values(by='value', ascending=False)[ptyp])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {
    "id": "e358f5c9"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- This graph shows an interesting aspect. The number of orders are high in Type1 and less in Departmental stores.\n",
    "- Even though the number of orders are less in Departmental because of high MRP the overall sales is higher than Type1.\n",
    "- It also shows the Type2 orders are very high. The average MRPs of the orders is very low compared to Departmental because it is not focusing on high MRPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {
    "id": "cbf806b6"
   },
   "source": [
    "### **Influence of Product_Allocated_Area on Product_Store_Sales_Total**\n",
    "- Product_Allocated_Area is giving good insights on possible improvments for the business.\n",
    "- The display area influences the sales. It should be usually directly proportional to the sales. If it is not observed to be proportional then it is indicative of some improvements for the business.\n",
    "- A detailed analysis is already in the section \"Understand Product Allocated Area and Heatmap section. We only do an overall influence analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {
    "id": "7c114655"
   },
   "source": [
    "#### **Overall influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "5a5af5a7",
    "outputId": "bf7a05cb-b127-4f8e-8300-322f93bd874e"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=skart_long, x=paa+\"_mean\", y=target+\"_sum\", hue=styp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {
    "id": "97488871"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- This is looking different plot from MRP and Weight.\n",
    "- It shows as if it is a flat strcuture with NOTHING correlated.\n",
    "- **This is because of distribution and usage of Area allocation by the stores. W.r.t MRP and Weights, the stores do not much of leverage to play around with. They take products that match range of MRPs in the  market segement which they are focused. But w.r.t Area allocation they are having the choice for controlling the display area in store for the products. Therefore we see a lot of overlap. Higher Weights and MRPs are using lower display areas. And vice versa**\n",
    "- **Also this is Area where improvements have been identified. Store Business have the flexibility to change this and must use the analysis to allocate areas as per the sales. Primarily due to mized up area allocations across MRPs the correlations to sales shows as linear in the graph. but we take overall aggreagete across all products and stores is weakly correlated with sales just like MRPs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {
    "id": "149a42fe"
   },
   "source": [
    "#### **Product and Store wise influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e28ec4ed",
    "outputId": "7f08aa14-9824-454c-d36c-161ee2e9e8e5"
   },
   "outputs": [],
   "source": [
    "FScatter(skart, ptyp, styp, psc, paa, target,ht=3, asp=1, fnt=7, tit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {
    "id": "274aaff6"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- The higher points are on the Deparmental stores and least is Food Market.\n",
    "- All points are pretty flat. No Correaltions with Target variables.\n",
    "- But w.r.t target we notice a lot of anamolies which are explained in detail \"Transform dataset Analysis\" and heatmap sections.\n",
    "- **As we see in the heatmap there are -ve correlations or no correlations for some product and store types for the Area. There are no +ve correlations. Efen the -ve correlations are mainly attributed to lesser orders and not directly realted to Area**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {
    "id": "09c6f5dd"
   },
   "source": [
    "### **Influence of **Product_Sugar_Content** on **Product_Store_Sales_Total****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {
    "id": "2967a039"
   },
   "source": [
    "#### **Overall influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1148fa53",
    "outputId": "c16ec0e3-84b4-48a1-cd91-311710696732"
   },
   "outputs": [],
   "source": [
    "cross_tab(skart_cat, target_bin, psc, nrows=2, ncols=1, figsize=(15,10), stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {
    "id": "8bdb73dd"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- In general, irrespective of Product type or Store type \"Low Sugar\" is selling well.\n",
    "- No Sugar and Regular are by far less in percentage sales. Regular is higher than No Sugar.\n",
    "- This is true for all MRPs and ranges of sales as confirmed by the graph above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {
    "id": "5534f930"
   },
   "source": [
    "#### **Store wise influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6aa9ad56",
    "outputId": "09ee9187-16c3-4a23-a408-1399e33a76e7"
   },
   "outputs": [],
   "source": [
    "cross_tab(skart_cat, psc, styp, nrows=2, ncols=1, figsize=(15,10), stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {
    "id": "b9d00cde"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Nothing new here. It only shows the stores and higher the sales of store higher the sales of \"Low Sugar\" products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161",
   "metadata": {
    "id": "801d68a5"
   },
   "source": [
    "#### **Product and Store wise influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "id": "31dfde97",
    "outputId": "57f0ff9e-f615-49c8-ec82-e16ebf09558f"
   },
   "outputs": [],
   "source": [
    "g = sns.catplot(data=skart_cat, kind='count', y=ptyp, hue=styp, col=psc, orient='h', height=9, aspect=0.5)\n",
    "g.set(xlim=(0, 350 * 1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {
    "id": "341a6cc1"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- The graph is illustrating the Product_Sugar_Content across products and store types\n",
    "- It shows that (left graph) \"No Sugar\" category is mutually exclusive applied to non consumable products and not for food items.\n",
    "- Only Household, Health and Hygiene and Other are \"No Sugar\".\n",
    "- As we already know \"Low Sugar\" has higher demand which is shown in the 2nd and 3rd graphs.\n",
    "- There is nothing specific about Stores w.r.t Sugar content. It is only the products. All stores are having similar behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {
    "id": "a838653f"
   },
   "source": [
    "### **Influence of Product_Weight on Product_Store_Sales_Total**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {
    "id": "50e4d163"
   },
   "source": [
    "#### **Overall influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "a51d4553",
    "outputId": "27324c9b-c6a7-429a-c4fb-0842f1ab9fed"
   },
   "outputs": [],
   "source": [
    "g = sns.scatterplot(data=skart, x=pwt, y=target, hue=styp)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(0,0), title=styp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {
    "id": "8f5e87f9"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Shows a nice linear corrleation between sales and Weight\n",
    "- It is a +ve correlation shown by gradual increase of the shape. It is very distinct for the Type1 and Type2 supermarkets.\n",
    "- For Food Market and Departmental the scatter point right and left respectively acts as outliers and reduce the linear correlation and hence the heatmap is showing lesser number of correlation. But this is not in anyway an actionable. It is only an insight that the Food Market and Departmental Stores are seeling product that have lower Weights with higher sales and/or vice versa.\n",
    "- Departmental stores is on the top, Food market at bottom. Type1 and Type2 in the middle like the MRP graph.\n",
    "- refer to MRP and heatmap, Transformed dataset sections. A very detailed analysis is presented there on this.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {
    "id": "ab5ea417"
   },
   "source": [
    "#### **Store wise influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f3db6534",
    "outputId": "783f0c3c-77e3-487a-a9f2-49744dcf3f52"
   },
   "outputs": [],
   "source": [
    "cross_tab(skart_cat, pwt_bin, styp, nrows=2, ncols=1, figsize=(15,10), stacked=True, margins=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {
    "id": "83809123"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- This is a similar beahviour as we see for MRP.\n",
    "- In lower Quartiles we see is dominated by Food MArket Stores.\n",
    "- In Middle and Upper middle are dominated by Type2 SuperMarket\n",
    "- In the higher quartiles we more of Type1 and Departmental stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171",
   "metadata": {
    "id": "6f97af1b"
   },
   "source": [
    "#### **Product and Store wise influence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7d91b8a0",
    "outputId": "6a7b742f-b1e7-463e-e562-6dd6b5af7f92"
   },
   "outputs": [],
   "source": [
    "FScatter(skart, ptyp, styp, psc, pwt, target,ht=3, asp=1, fnt=7, tit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {
    "id": "f15f1150"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Pretty much similar patterns that we have already seen in Product MRP.\n",
    "- The higher dots correspond to Departmental stores and lower dots to Food MArket.\n",
    "- Type 1 and Type 2 follow below the Departmental.\n",
    "- Overall Strong +Ve correlation with Sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174",
   "metadata": {
    "id": "4d4fd159"
   },
   "source": [
    "### **Influence of **Store_Establishment_Year** on **Product_Store_Sales_Total****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dfcd990b",
    "outputId": "ead31c7c-5960-41d4-ddc5-4d5f3f519204"
   },
   "outputs": [],
   "source": [
    "cross_tab(skart_cat, target_bin,seyr, nrows=2, ncols=1, figsize=(15,10), stacked=True, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {
    "id": "55c2079a"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Simply shows higher sales for Store established in 2009. Which is the Type2 Super Market.\n",
    "- One can interpret this as new Ambience and Mall like setup will influence the customers more tocome to store. Therefore the stores built later may have such an ambience.\n",
    "- IF it is true the business can evaluate the renovation option for incerasing the sales.\n",
    "- Other than this aspect there is nothing new in the analysis for this fearture.\n",
    "- It also shows 1987 the first store was established."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177",
   "metadata": {
    "id": "4cae9326"
   },
   "source": [
    "### **Influence of **Store_Location_City_Type** on **Product_Store_Sales_Total****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178",
   "metadata": {
    "id": "0de24751"
   },
   "source": [
    "#### **Influence on MRP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe5961b8",
    "outputId": "88e4d65d-6893-4081-8c0b-99e919c27c8f"
   },
   "outputs": [],
   "source": [
    "skart_long.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "id": "76092c22",
    "outputId": "a4527651-d50f-431e-9d6e-c93d8c4ade4f"
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=skart_long, kind='bar', y=ptyp, hue=styp, x=pmrp+\"_mean\", col=sloctype, orient='h', height=9, aspect=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "id": "53f7cc53",
    "outputId": "c861b611-4b6e-4c99-97b6-7a78659623d4"
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=skart_long, kind='bar', y=ptyp, hue=styp, x=pwt+\"_mean\", col=sloctype, orient='h', height=9, aspect=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182",
   "metadata": {
    "id": "577f8e4a"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- The above 2 graphs shows Weights/MRPs. They are smaller for Tier 3 and highest for Tier 1 and in the middle for Tier 2.\n",
    "- This increase of MRP in Tiers is not fully because of Tiers. As we saw the increase in MRP also is affected by the Weights.\n",
    "- Tier 3's are selling smaller weights, In Tier 2 the Type 1 SuperMarket is selling higher weight and Type 2 is selling lower weights.\n",
    "- Tier 1 is only selling high weights.\n",
    "- Therefore there the  increase in MRP, Tier 1 > Tier 2 > Tier 3 is a mix of Tiers (cost of living) and Weights of the products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183",
   "metadata": {
    "id": "0610994a"
   },
   "source": [
    "### **Influence of **Store_Size** on **Product_Store_Sales_Total****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "f4d79873",
    "outputId": "e162a665-4436-4a27-b8c7-1f588f739769"
   },
   "outputs": [],
   "source": [
    "cross_tab(skart_cat, target_bin, ssize, nrows=1, ncols=2, figsize=(15,5), stacked=True, margins=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {
    "id": "fb822af2"
   },
   "source": [
    "#### **OBSERVATIONS**\n",
    "- The graph shows expected output. In the dataset we have Type1 super market as \"High\". It is selling higher MRP products and has sales only in the higher quartile.\n",
    "- The Type2 which is having higher sales is selling middle and upper middle auartiles.\n",
    "- the Food Market is seeling lower quartiles.\n",
    "- The departmental store is selling very high quartiles than the Type1 store is also size \"Medium\" in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "id": "147f6eaf",
    "outputId": "fb7ffa61-9313-450b-f5c1-f7d3c674ab24"
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=skart_long, kind='bar', y=ptyp, hue=styp, x=paa+\"_mean\", col=ssize, orient='h', height=9, aspect=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187",
   "metadata": {
    "id": "89cd21b7"
   },
   "source": [
    "#### **OBSERVATIONS**\n",
    "- The graph shows output of how the stores exercising their freedom in choosing the product allocation areas based on the stores sizes.\n",
    "- One would assume that the stores have analyzed the sales and came up with this allocations. However certain anamolies were observed wherein store are allocating more area but sales are not in proportion. This was detailed unders the section \"Understanding Product Allocation Area\"\n",
    "- Display area is primiarly influencing factor of the sales and size of store dictates how much area is avilable. Therefore it influences the over all sales.\n",
    "- In the pie charts detailed in \"Understanding Product Allocation Area\" it shows that a majority of product types are allocated correctly area and are resulting +ve sales to Area Allocation Ratio. Only a handfull of product types are -vely correlated which the business can analyze. As it is was explained earleir this graph here is only for completness. Please refere the sections mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {
    "id": "ba53c762"
   },
   "source": [
    "### Influence of **Store_Type** on **Product_Store_Sales_Total**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "id": "V4I4kXz0qHQf",
    "outputId": "b07c21f0-8e79-4cac-992e-578c2b42f3c9"
   },
   "outputs": [],
   "source": [
    "sns.catplot(data=skart_long, kind='bar', y=ptyp, hue=styp, x=target+\"_sum\", col=styp, orient='h', height=9, aspect=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {
    "id": "c1418f1f"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- The sales across stores is known that type2 is highest and Food Market is lowest.\n",
    "- Fruits and vegetables are the highest selling products.\n",
    "- The interesting thing to note here is that, apart from minor varitions in products like Dairy and Meat, (in which Depratmental and Type1 relative sales percentages are higher than their counter parts in Food MArket and Type2), overall the product sales pattern is same across all the stores. Relative percentage of sales is almost same across all the stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191",
   "metadata": {},
   "source": [
    "# **insights based on EDA - Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192",
   "metadata": {},
   "source": [
    "- The 4 stores in the dataset are having mutually exclusive products. They are meant for different market segementations. By design the store types are designed for addressing differnt needs of the customers.\n",
    "- The supermarkets Type2 addresses daily consumption products which are most commonly purchased by every household on a regualr basis. It targets lower quartile to Middle and Upper middle qaurtiles of the MRPS segements for the products.\n",
    "- Type1 supermarket products are an intelligent mix of Premium products and a Selected set upper Middle quartile of frequently used products. They have an MRP range above the Super Markets but lesser than Depratmental stores.\n",
    "- The Departmental Stores is more Premium Product Segement. It targets higher MRPs. Even if there are fewer number of orders the overall sales will be much higher than the supermarkets like Type1. They address Tier 1 cities.\n",
    "- The food Market is like convienence store for \"Small\" stores addressing very low MRPs for products.\n",
    "- The products are classified into 16 different types. Broadly there are Food items, Drinks and Non Consumables. Out of these items the Food items comprise a large portion of sales, the highest being Fruits and Vegetables for all the stores.\n",
    "- the key attributes for products are\n",
    "   - Weight: Product Weight\n",
    "        - It is observed that lower Middle, Middle and Upper Middle Quartiles the Weight of the product is highly correlated with the sales positiely. For lower Qaurtiles and Upper / Outlier quarties Weight is not a well correlated, meaning lower weight can have higher MRP and vice versa.\n",
    "        - In general Super market are focused in this MRP segements hence the correlation of Weights and sales is very high for Super Markets.\n",
    "   - MRP: the seeling price of the product\n",
    "        - By design it is qualifying the Store Type. The 4 Stores types are addressing differnt MRP segements.\n",
    "   - Area: display Area allocated by the product.\n",
    "        - totally controlled by Store adminstration to maximize sales. It succeded in more than 90% of the cases but there a few cases where Areas aalocated are -vely proportion to sales. The business have to take these on case by cases and adjust the allocation areas.\n",
    "        - The heatmap section of this report details the products and store combination where -ve correlations are observed.\n",
    "- The stores by design are addressing different MRPs. They monitor sales and take there own decisions on how to adjust the Display allocation Area for the products so that sales is maximized. The product Weights influence the sales as the Area and MRPs are correlated to the Weights.\n",
    "- The correlation of Product attributes with Sales is a very complex relationship it is difficult to visualize. The Heatmap and Pairplot provide ver weak insights. This is a perfect ase for an ensemble model to find out deeper realtionships and convert this into ML model.\n",
    "- Another Attribute of the product is Product Sugar content level. It is fairly straght forward showig high sales for all products which are \"Low Sugar\". The sales of \"Regular\" sugar content is relatively low compared to \"Low Sugar\". this attribute also classifies \"Non Consumable\" products as \"No Sugar\"\n",
    "- The store attributes are more straight forward\n",
    "   - Store Location tpye: It is indicative for which Tier the Store is going to Target. Tier 1 for high cost of living areas and Tier 3 for low cost of living. tier 2 is in between. In the dataset provided  Super Market type store are in \"Tier 2\", Departmental Stores is in \"Tier 1\" and Food MArket is in \"Tier 3\"\n",
    "   - Store Size classifies stores as \"High\", \"Medium\" and \"Small\". Thisis simply reflective of the area of store it is not anyway  reflective of Allocation Areas and/or MRPs. IN the Dataset provided \"High\" is a \"Tier 2\" Super Market Type 1. Medium is Departmental and Super Market Type2. Tier 3 Food Market is \"Small\"\n",
    "   - Store Establishment Year simply gives in which year the store was established. It is observed that the store established in 1987 had very good management of MRPs Vs Weight Vs Area and correlations of this Type 1 Vintage 1987 store was the highest with Sales. Other stores must adopt the best practices of allocation Areas and Choosing MRPs / Weight from this Store.\n",
    "   -overall one can say the sales prediction is complex combinations of product and Store attributes. The model has to consider all  all Product and Stores attributes. There are many hidden relationships between Store and Product attributes which are best enalyzed by ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {
    "id": "0fo5OvIfVdtB"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194",
   "metadata": {
    "id": "7847cb90"
   },
   "source": [
    "## **Feature Engineering - Part2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195",
   "metadata": {
    "id": "1c23bf06"
   },
   "source": [
    "No Addtional feature Engineering is required here. We have already completed the following in during the EDA\n",
    "- Added a new Column derived from the Product_ID first two characters. This helps is classifying the items as Food, Drinks and Non Consumables which gives gives insights into Sales per these categories. However, after analysis it is not adding any value. As we saw in the previous section the relative product percentage sales are almost similar across all the categories so putting them in an in a different product type category is not necessary.\n",
    "- Morevoer the minor deviation in some pattern like Dairy, Meat across stores can be stected by ensemble models and more accruate predictions are possible. Therefore we drop the new column added.\n",
    "- Added Numerical bins based on Quartiles, Outliers and IQRs. This is helping EDA analysis but in Model building it is decided to drop them as they do not add any values\n",
    "- In addtion we drop PID and SID. Explained below  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196",
   "metadata": {
    "id": "655f879e"
   },
   "source": [
    "### **Drop PID and SID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {
    "id": "30e9e66a"
   },
   "outputs": [],
   "source": [
    "X = skart.drop(columns=[target, pid, sid, pidc2])\n",
    "y = skart[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198",
   "metadata": {
    "id": "4b7cf245"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Product ID is unique ID for all the rows. Therefore this can now dropped as IDs which are unique for the dataset do not help in ML.\n",
    "- Store ID is useful as it explains relationship between Products and Stores and helps in Aggregations and identifying patterns. However, Store_Type in the dataset uniquely idenitifies stores. Therefore Store ID is redundant. Therefore even Store ID is dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199",
   "metadata": {
    "id": "3a1c8ae4"
   },
   "source": [
    "### **Outlier Treatments - Options and Decision**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200",
   "metadata": {
    "id": "c726895b"
   },
   "source": [
    "##### **OBSERVATIONS** - OPTIONS\n",
    "The applicable options available for outlier treatment here are:\n",
    "- Cap the Outliers at the upper and lower bounds as calculated by the Tukey method.\n",
    "- Leave them as is as they are meaningfull and lead to some useful patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201",
   "metadata": {
    "id": "428ab3be"
   },
   "source": [
    "##### **OBSERVATIONS** - DECISIONS\n",
    "- IN THIS CASE IT IS WISE TO LEAVE OUTLIERS AS IS AND ANALYZE THEM SPECIFICALLY IF THERE ANY PATTERN SPECIFIC TO OUTLIERS.\n",
    "IN CASE OF SUPERMARKETS THERE COULD MANY EVENTS, CELEBRITY VISITS, SALE CAMPAIGNS WHICH WILL SPIKE SALES ON A FEW DAYS.\n",
    "DURING SUCH SPIKES IT IS MORE IMPORTANT TO ANALYZE THE PATTERN OF SALES ON WHAT PEOPLE TEND BUY RATHER THAN IGNORE THEM.\n",
    "CAPPING OUTLIERS WILL LOOSE DATA THAT IS IMPORTANT.\n",
    "- TO ANALYZE OUTLIERS WE WILL MAKE BINS (USING pd.cut) WITH BIN EDGES MATCHING THE IQR, QUANTILE AND OUTLIER RANGES.\n",
    "WE THEN VISUALIZE FOR ANY SPECIFIC PATTERNS WITH OUTLIERS IN THE BI-VARIATE ANALYSIS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202",
   "metadata": {
    "id": "56b60df4"
   },
   "source": [
    "## **Split the dataset into Testing, Validation and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {
    "id": "9d99f338"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Train / Val / Test split\n",
    "# -----------------------------\n",
    "# 60% train, 20% val, 20% test\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42\n",
    ")  # 0.25 of 0.8 -> 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204",
   "metadata": {
    "id": "47e7c987"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- We take a 3 way split into Train, Validation and Testing.\n",
    "- This is required because we intend to Hyper Tune the parameters of the Models.\n",
    "- We will fit the Models first with Train and Then Hyper Tune with Val.\n",
    "- finally we verify with Test. This way we prevent data leaks by keeping Val and Test Sperate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205",
   "metadata": {
    "id": "3cc10eeb"
   },
   "source": [
    "## **Prepare a single pipeline for Categorical and Numeric variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5131e91f",
    "outputId": "5a7fcfe9-5ad5-4ea7-ae1c-76157e6b268a"
   },
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X_train.select_dtypes(exclude=[\"int64\", \"float64\"]).columns  # Already ordinal encoded\n",
    "print ( numeric_features)\n",
    "print( categorical_features)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', RobustScaler(), numeric_features),\n",
    "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207",
   "metadata": {
    "id": "08e0042e"
   },
   "source": [
    "##### **OBSERVATIONS**\n",
    "- Scale the numeric values using Robust Scalar\n",
    "- Robust Scalar is used as all the numeric values have outliers.\n",
    "- This pipeline will be auto applied on Testing and Validation datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208",
   "metadata": {
    "id": "5fd3cabe"
   },
   "source": [
    "# **Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209",
   "metadata": {
    "id": "718531d9"
   },
   "source": [
    "## **Metrics Used - Regression**\n",
    "- Relavent metrics used are RMSE, R2 and Adj R2\n",
    "- As this is Regression problem we minimize RMSE value and fine tune Hyper parameters to get the best value or R2 close to 1\n",
    "- Also make sure we have good value of Adj R2 such that we dont have variables in the dataset not contributing to error reductions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210",
   "metadata": {
    "id": "YyzOQ8pBY93N"
   },
   "source": [
    "## **Define functions for Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {
    "id": "d107c3d3"
   },
   "outputs": [],
   "source": [
    "# function to compute adjusted R-squared\n",
    "def adj_r2_score(predictors, targets, predictions):\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    n = predictors.shape[0]\n",
    "    k = predictors.shape[1]\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "\n",
    "# function to compute different metrics to check performance of a regression model\n",
    "def model_performance_regression(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check regression model performance\n",
    "\n",
    "    model: regressor\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    r2 = r2_score(target, pred)  # to compute R-squared\n",
    "    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n",
    "    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n",
    "    mae = mean_absolute_error(target, pred)  # to compute MAE\n",
    "    mape = mean_absolute_percentage_error(target, pred)  # to compute MAPE\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R-squared\": r2,\n",
    "            \"Adj. R-squared\": adjr2,\n",
    "            \"MAPE\": mape,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212",
   "metadata": {
    "id": "P14pbR8nAefF"
   },
   "source": [
    "The ML models to be built can be any two out of the following:\n",
    "1. Decision Tree\n",
    "2. Bagging\n",
    "3. Random Forest\n",
    "4. AdaBoost\n",
    "5. Gradient Boosting\n",
    "6. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213",
   "metadata": {
    "id": "FtkIDTjdYy5h"
   },
   "source": [
    "# **Model Performance Improvement - Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214",
   "metadata": {
    "id": "76eaa538"
   },
   "source": [
    "## **Run default models first and select the best two models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFr066WFqVmT",
    "outputId": "2d9664d1-8b70-4c86-83db-87eac238fe68"
   },
   "outputs": [],
   "source": [
    "# List of models to test\n",
    "models = {\n",
    "    \"AdaBoost\": AdaBoostRegressor(random_state=42),\n",
    "    \"GradientBoost\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42, verbosity=0),\n",
    "    \"Bagging\": BaggingRegressor(random_state=42),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42)\n",
    "}\n",
    "import time\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    print( \" Processing Model = \", name)\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    start_fit_predict = time.perf_counter()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    res_train = model_performance_regression(pipeline, X_train, y_train)\n",
    "    time_taken = round( (time.perf_counter() - start_fit_predict), 2)\n",
    "    res_train['Type'] = 'Training'\n",
    "    res_train['Time'] = time_taken\n",
    "\n",
    "    res_val = model_performance_regression(pipeline, X_val, y_val)\n",
    "    res_val['Type'] = 'Validation'\n",
    "\n",
    "    res_test = model_performance_regression(pipeline, X_test, y_test)\n",
    "    res_test['Type'] = 'Testing'\n",
    "    total_time_taken = round ( (time.perf_counter() - start_fit_predict), 2)\n",
    "    res_val['Time'] = total_time_taken\n",
    "    res_test['Time'] = total_time_taken\n",
    "\n",
    "    one_model = pd.concat([res_train, res_val, res_test], ignore_index=True)\n",
    "    one_model['Name'] = name\n",
    "    results.append(one_model)\n",
    "    print (\" Finished model = \", name, \" time taken Train:Total\", time_taken, total_time_taken)\n",
    "\n",
    "print( \"Completed all Models = \", len(results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "0c14f3d5",
    "outputId": "47986c50-afc3-41d7-9317-8ca002900d8f"
   },
   "outputs": [],
   "source": [
    "all_results = pd.concat(results, ignore_index=True)\n",
    "all_results.sort_values(by=['Time', 'Adj. R-squared'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217",
   "metadata": {
    "id": "dc0ddbc3"
   },
   "source": [
    "### **OBSERVATIONS**\n",
    "1. Performance of Random Forest is very bad inspite of having better R2 and Adj R2 it is not preferred. Performance is an important aspect and model must be able train faster. The time taken by the model is shown in the \"Time\" column.\n",
    "2. Gradient Boost scores are very close but again performance is very bad. Therefore this is also not shortlisted.\n",
    "3. AdaBoost has good performance results but scores are bad. Therefore this is also rejected.\n",
    "4. The best models in terms of performance and Metrics are XGBoost and Bagging.\n",
    "5. However, in both Bagging and XGBoost the Test and Train R2 values have 7% difference ( 91 Vs 98). This is case for Hyper Tuning. So lets Hyper tune.\n",
    "\n",
    "**Lets Hyper parameter tune the Models XGBoost and Bagging**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218",
   "metadata": {
    "id": "bd87c2c6"
   },
   "source": [
    "## **Hyper parameter Tuning for the top 2 models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc2d3b47",
    "outputId": "fa8e2915-503e-4489-f7fd-1dc01c767859"
   },
   "outputs": [],
   "source": [
    "# Define models and their parameter grids\n",
    "model_params = {\n",
    "    \"Bagging\": {\n",
    "        \"model\": BaggingRegressor(random_state=42),\n",
    "        \"params\": {\n",
    "            \"model__n_estimators\": [50, 100, 200],\n",
    "            \"model__max_samples\": [0.5, 0.8, 1.0],\n",
    "            \"model__max_features\": [0.5, 0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBRegressor(random_state=42, eval_metric=\"rmse\"),\n",
    "        \"params\": {\n",
    "            \"model__n_estimators\": [100, 200],\n",
    "            \"model__max_depth\": [3, 5, 7],\n",
    "            \"model__learning_rate\": [0.01, 0.1, 0.2],\n",
    "            \"model__subsample\": [0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results = []\n",
    "models_best = []\n",
    "\n",
    "for name, mp in model_params.items():\n",
    "    print(f\"\\n🔹 Tuning {name}...\")\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", mp[\"model\"])\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=mp[\"params\"],\n",
    "        scoring=\"r2\",\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # grid = RandomizedSearchCV(\n",
    "    #     estimator=pipeline,\n",
    "    #     param_distributions=mp[\"params\"],\n",
    "    #     scoring=\"r2\",\n",
    "    #     cv=5,\n",
    "    #     n_jobs=-1,\n",
    "    #     verbose=1\n",
    "    # )\n",
    "\n",
    "    start_time = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    elapsed = round( (time.time() - start_time),2)\n",
    "    best_score = round(grid.best_score_, 4)\n",
    "\n",
    "    print(f\"Time taken: {elapsed:.2f} seconds\")\n",
    "    print(f\"Best Params: {grid.best_params_}\")\n",
    "    print(f\"Best CV Score: {best_score:.4f}\")\n",
    "\n",
    "    # Evaluate best model\n",
    "    best_model = grid.best_estimator_\n",
    "    models_best.append(best_model)\n",
    "\n",
    "    for X_set, y_set, set_name in [(X_train, y_train, \"Training\"),\n",
    "                                   (X_val, y_val, \"Validation\"),\n",
    "                                   (X_test, y_test, \"Testing\")]:\n",
    "        res = model_performance_regression(best_model, X_set, y_set)\n",
    "        res[\"Type\"] = set_name\n",
    "        res[\"Name\"] = name\n",
    "        res[\"Time\"] = elapsed\n",
    "        results.append(res)\n",
    "    print ( \"Completed Model name:score:Time = \", name, best_score, elapsed)\n",
    "print(\"Complete Hyper Tuning for all models = \", len(results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220",
   "metadata": {
    "id": "b0810287"
   },
   "source": [
    "# **Model Performance Comparison, Final Model Selection, and Serialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221",
   "metadata": {
    "id": "bad66a03"
   },
   "source": [
    "## **Performance Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "ec6a7bb9",
    "outputId": "02deecee-e2c3-45c0-af96-772001592a10"
   },
   "outputs": [],
   "source": [
    "# Combine results\n",
    "final_results = pd.concat(results, ignore_index=True)\n",
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223",
   "metadata": {
    "id": "79d0c4da"
   },
   "source": [
    "## **Final Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224",
   "metadata": {
    "id": "7b1e2134"
   },
   "source": [
    "### **OBSERVATIONS**\n",
    "1. XGBoost is clear winner in terms of performance.\n",
    "2. Bagging test scores exceed XGBoost by 0.3%. But this difference is very minimal.\n",
    "3. The difference between testing and Training score is higher for Bagging. 93% Vs 97% = 4%. For XGBoost it is lower 93% Vs 95% = 2%. This shows that XGBoost has generalized better than Bagging.\n",
    "\n",
    "****Final Model Selected = XGBoost****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225",
   "metadata": {
    "id": "26b51c5d"
   },
   "source": [
    "# **Feature importance - XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "6cfbc902",
    "outputId": "41c1ef10-ab3f-4e7b-8a16-818527965446"
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_model =models_best[1]\n",
    "\n",
    "feature_importances = pd.Series(\n",
    "    best_model.named_steps['model'].feature_importances_,\n",
    "    index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(feature_importances)\n",
    "\n",
    "plot_importance(best_model.named_steps['model'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227",
   "metadata": {
    "id": "a39f6f6c"
   },
   "source": [
    "## **Serialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228",
   "metadata": {
    "id": "1458f0a0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Create a folder to upload your trained serialized model into it\n",
    "os.makedirs(\"backend_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {
    "id": "X2M3eYFgqZEk",
    "outputId": "ea978373-1054-47ed-f3cc-7af46e0555ae"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Save the model\n",
    "joblib.dump(models_best[1], \"backend_files/final_xgb_pipeline.joblib\")\n",
    "print(\"Model saved as final_xgb_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230",
   "metadata": {
    "id": "b35109e9"
   },
   "source": [
    "### **Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231",
   "metadata": {
    "id": "0672f0c6",
    "outputId": "9f6c639a-1b96-4fa7-c7cf-60672cb55e1b"
   },
   "outputs": [],
   "source": [
    "# load it\n",
    "loaded_model = joblib.load(\"backend_files/final_xgb_pipeline.joblib\")\n",
    "\n",
    "# Test loaded model\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "print(\"Test R² after reload:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Now get the preprocessor step\n",
    "preprocessor = loaded_model.named_steps[\"preprocessor\"]\n",
    "\n",
    "# Extract numeric and categorical column names\n",
    "num_features = preprocessor.transformers_[0][2]\n",
    "cat_features = preprocessor.transformers_[1][2]\n",
    "\n",
    "print(\"Numeric Features:\", num_features)\n",
    "print(\"Categorical Features:\", cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232",
   "metadata": {
    "id": "9a2LCguV-7i1"
   },
   "source": [
    "# **Deployment - Backend**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233",
   "metadata": {
    "id": "T3XlDPUtJnDo"
   },
   "source": [
    "## Flask Web Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234",
   "metadata": {
    "id": "D_ibq6X2qkZ0",
    "outputId": "a3f80f61-743d-4f82-d5b3-4614fd13e868"
   },
   "outputs": [],
   "source": [
    "%%writefile backend_files/app.py\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import joblib  # For loading the serialized model\n",
    "import pandas as pd  # For data manipulation\n",
    "from flask import Flask, request, jsonify  # For creating the Flask API\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# print( \" Trying to load XGBoost model using joblib\")\n",
    "model = joblib.load(\"final_xgb_pipeline.joblib\")\n",
    "\n",
    "# print(\"Model loaded successfully!\")\n",
    "\n",
    "# Initialize the Flask application\n",
    "sales_predictor_api = Flask(\"SuperKart Sales Prediction\")\n",
    "\n",
    "# Define a route for the home page (GET request)\n",
    "@sales_predictor_api.get('/')\n",
    "def home():\n",
    "    \"\"\"\n",
    "    This function handles GET requests to the root URL ('/') of the API.\n",
    "    It returns a simple welcome message.\n",
    "    \"\"\"\n",
    "    return \"Welcome to the SuperKart Sales Prediction API!\"\n",
    "\n",
    "# Define an endpoint for single property prediction (POST request)\n",
    "@sales_predictor_api.post('/v1/sales')\n",
    "def predict_sales():\n",
    "    \"\"\"\n",
    "    This function handles POST requests to the '/v1/sales' endpoint.\n",
    "    It expects a JSON payload containing property details and returns\n",
    "    the predicted rental price as a JSON response.\n",
    "    \"\"\"\n",
    "    # Get the JSON data from the request body\n",
    "    property_data = request.get_json()\n",
    "\n",
    "    # Extract relevant features from the JSON data\n",
    "    sample = {\n",
    "\n",
    "        'Product_Weight': property_data['Product_Weight'],\n",
    "        'Product_Allocated_Area': property_data['Product_Allocated_Area'],\n",
    "        'Product_MRP': property_data['Product_MRP'],\n",
    "        'Product_Sugar_Content': property_data['Product_Sugar_Content'],\n",
    "        'Product_Type': property_data['Product_Type'],\n",
    "        'Store_Establishment_Year': property_data['Store_Establishment_Year'],\n",
    "        'Store_Size': property_data['Store_Size'],\n",
    "        'Store_Location_City_Type': property_data['Store_Location_City_Type'],\n",
    "        # 'pid_c2': property_data['pid_c2'],\n",
    "        'Store_Type': property_data['Store_Type']\n",
    "    }\n",
    "    # print( ' recevied request from client ')\n",
    "    # Convert the extracted data into a Pandas DataFrame\n",
    "    input_data = pd.DataFrame([sample])\n",
    "    # print(\"data recevied = \", input_data)\n",
    "    # Make prediction (get log_price)\n",
    "    predicted_sales = model.predict(input_data)[0]\n",
    "    predicted_sales = float(predicted_sales)  # convert to native float\n",
    "    # print (\"Sales predicted = \", predicted_sales)\n",
    "    return jsonify({'Predicted Sales': predicted_sales})\n",
    "\n",
    "# Run the Flask application in debug mode if this script is executed directly\n",
    "if __name__ == '__main__':\n",
    "    sales_predictor_api.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235",
   "metadata": {
    "id": "STDSb04iT-rL"
   },
   "source": [
    "## **Dependencies File**\n",
    "- Some more dependencies are there required for REST based model\n",
    "- Having these here will alow the backend to be used as UI + Backend without REST. So keeping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236",
   "metadata": {
    "id": "tZbPjRESqlf2",
    "outputId": "e28f8738-4508-4a60-cdf1-336a7dee20aa"
   },
   "outputs": [],
   "source": [
    "%%writefile backend_files/requirements.txt\n",
    "pandas==2.2.2\n",
    "numpy==2.0.2\n",
    "scikit-learn==1.6.1\n",
    "xgboost==2.1.4\n",
    "joblib==1.4.2\n",
    "Werkzeug==2.2.2\n",
    "flask==2.2.2\n",
    "gunicorn==20.1.0\n",
    "requests==2.28.1\n",
    "uvicorn[standard]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237",
   "metadata": {
    "id": "JWD7rPCRUEtD"
   },
   "source": [
    "## Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238",
   "metadata": {
    "id": "6UIbI8d9qmAe",
    "outputId": "99872899-1b4c-4c2f-aeac-c8ce5223fd4d"
   },
   "outputs": [],
   "source": [
    "%%writefile backend_files/Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set the working directory inside the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy all files from the current directory to the container's working directory\n",
    "COPY . .\n",
    "\n",
    "# Install dependencies from the requirements file without using cache to reduce image size\n",
    "RUN pip install --no-cache-dir --upgrade -r requirements.txt\n",
    "\n",
    "# Define the command to start the application using Gunicorn with 4 worker processes\n",
    "# - `-w 4`: Uses 4 worker processes for handling requests\n",
    "# - `-b 0.0.0.0:7860`: Binds the server to port 7860 on all network interfaces\n",
    "# - `app:app`: Runs the Flask app (assuming `app.py` contains the Flask instance named `app`)\n",
    "CMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:7860\", \"app:sales_predictor_api\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239",
   "metadata": {
    "id": "yK1n7jBcRrYr"
   },
   "source": [
    "## Setting up a Hugging Face Docker Space for the Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240",
   "metadata": {
    "id": "IYOJipQ3qmrD"
   },
   "outputs": [],
   "source": [
    "# Import the login function from the huggingface_hub library\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to your Hugging Face account using your access token\n",
    "# Replace \"YOUR_HUGGINGFACE_TOKEN\" with your actual token\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # loads from .env\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if token is None:\n",
    "    raise ValueError(\"HUGGINGFACE_TOKEN not set in environment\")\n",
    "# print(token)\n",
    "\n",
    "login(token=token)\n",
    "\n",
    "# Import the create_repo function from the huggingface_hub library\n",
    "from huggingface_hub import create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241",
   "metadata": {
    "id": "2936658f",
    "outputId": "4d335a38-80ce-41ac-c3a2-26ca739a0805"
   },
   "outputs": [],
   "source": [
    "# Try to create the repository for the Hugging Face Space\n",
    "repoid = 'surnellas/SuperKart_Backend'\n",
    "try:\n",
    "    create_repo(\"SuperKart_Backend\",  # One can replace \"Backend_Docker_space\" with the desired space name\n",
    "        repo_type=\"space\",  # Specify the repository type as \"space\"\n",
    "        space_sdk=\"docker\",  # Specify the space SDK as \"docker\" to create a Docker space\n",
    "        private=False  # Set to TrueCall.  if you want the space to be private\n",
    "    )\n",
    "except Exception as e:\n",
    "    # Handle potential errors during repository creation\n",
    "    if \"RepositoryAlreadyExistsError\" in str(e):\n",
    "        print(\"Repository already exists. Skipping creation.\")\n",
    "    else:\n",
    "        print(f\"Error creating repository: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242",
   "metadata": {
    "id": "B4tnVrlo8xQ9"
   },
   "source": [
    "## Uploading Files to Hugging Face Space (Docker Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243",
   "metadata": {
    "id": "naObvoxDqnMS",
    "outputId": "88f47e13-6c9f-4908-8859-7fa2ea8d37b1"
   },
   "outputs": [],
   "source": [
    "# for hugging face space authentication to upload files\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_id = \"surnellas/SuperKart_Backend\"  # Your Hugging Face space id\n",
    "\n",
    "# Initialize the API\n",
    "api = HfApi()\n",
    "\n",
    "# Upload Streamlit app files stored in the folder called deployment_files\n",
    "api.upload_folder(\n",
    "    folder_path=\"backend_files\",  # Local folder path\n",
    "    repo_id=repo_id,  # Hugging face space id\n",
    "    repo_type=\"space\",  # Hugging face repo type \"space\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244",
   "metadata": {
    "id": "bv07DWg0_G6L"
   },
   "source": [
    "# **Deployment - Frontend**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245",
   "metadata": {
    "id": "3J1woYZNGhXh"
   },
   "source": [
    "## Points to note before executing the below cells\n",
    "- Create a Streamlit space on Hugging Face by following the instructions provided on the content page titled **`Creating Spaces and Adding Secrets in Hugging Face`** from Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246",
   "metadata": {
    "id": "UsCYxkq_UL3Q"
   },
   "source": [
    "## Streamlit for Interactive UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247",
   "metadata": {
    "id": "aC_Py-S9qpsJ"
   },
   "outputs": [],
   "source": [
    "# Create a folder for storing the files needed for frontend UI deployment\n",
    "os.makedirs(\"frontend_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248",
   "metadata": {
    "id": "bc690e89",
    "outputId": "2e81a78f-8af8-4baa-df7d-58300a5cbcdd"
   },
   "outputs": [],
   "source": [
    "%%writefile frontend_files/app.py\n",
    "import streamlit as st\n",
    "import requests\n",
    "import datetime\n",
    "import joblib  # For loading the serialized model\n",
    "import pandas as pd  # For data manipulation\n",
    "from flask import Flask, request, jsonify  # For creating the Flask API\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import PowerTransformer, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Set the title of the Streamlit app\n",
    "st.title(\"SuperKart sales Prediction\")\n",
    "\n",
    "# Section for online prediction\n",
    "st.subheader(\"Online Prediction\")\n",
    "\n",
    "# cyear = datetime.now().year\n",
    "cyear=2025\n",
    "\n",
    "#--------------------------------------------TESTING (Model without REST)-----------------------------------------------\n",
    "\n",
    "# print( \" Trying to load XGBoost model using joblib\")\n",
    "# model = joblib.load(\"XGBoost_best_model.joblib\")\n",
    "\n",
    "# print(\"Model loaded successfully!\")\n",
    "\n",
    "#--------------------------------------------TESTING (Model without REST)-----------------------------------------------------\n",
    "\n",
    "# Collect user input for property features\n",
    "pwt = st.number_input(\"Product Weight\", min_value=0.1, value=100.0)\n",
    "paa = st.number_input(\"Product_Allocated_Area (Ratio of product area to Total Area)\", min_value=0.001, max_value=0.999, value=0.2)\n",
    "psc = st.selectbox(\"Product Sugar Content\", ['Low Sugar', 'Regular', 'No Sugar'])\n",
    "ptyp = st.selectbox(\"Product Type\", ['Fruits and Vegetables', 'Snack Foods', 'Frozen Foods', 'Dairy',\n",
    "       'Household', 'Baking Goods', 'Canned', 'Health and Hygiene',\n",
    "       'Meat', 'Soft Drinks', 'Breads', 'Hard Drinks', 'Others',\n",
    "       'Starchy Foods', 'Breakfast', 'Seafood'])\n",
    "ssize = st.selectbox(\"Store Size\", ['Small', 'Medium', 'High'])\n",
    "sloctype = st.selectbox(\"Store Location City Type\", ['Tier 1', 'Tier 2', 'Tier 3'])\n",
    "styp = st.selectbox(\"Store Type\", ['Supermarket Type2', 'Supermarket Type1', 'Departmental Store', 'Food Mart'])\n",
    "# pid_c2 = st.selectbox(\"pid_c2\", ['FD', 'DR', 'NC'])\n",
    "pmrp = st.number_input(\"Product MRP\", min_value=0.1,  value=100.0)\n",
    "seyr_i = st.number_input(\"Store Establishment Year\", min_value=1987, step=1, max_value=cyear, value=2025)\n",
    "seyr = str(seyr_i)\n",
    "\n",
    "# Convert user input into a DataFrame\n",
    "input_data = pd.DataFrame([{\n",
    "    'Product_Weight': pwt,\n",
    "    'Product_Allocated_Area': paa,\n",
    "    'Product_MRP': pmrp,\n",
    "    'Product_Sugar_Content': psc,\n",
    "    'Product_Type': ptyp,\n",
    "    'Store_Establishment_Year': seyr,\n",
    "    'Store_Size': ssize,\n",
    "    # 'pid_c2':pid_c2,\n",
    "    'Store_Location_City_Type': sloctype,\n",
    "    'Store_Type': styp\n",
    "}])\n",
    "\n",
    "# Make prediction when the \"Predict\" button is clicked\n",
    "if st.button(\"Predict\"):\n",
    "    print(\"payload = \", input_data.to_dict(orient='records')[0])\n",
    "    response = requests.post(\"https://surnellas-SuperKart-Backend.hf.space/v1/sales\", json=input_data.to_dict(orient='records')[0])\n",
    "    if response.status_code == 200:\n",
    "        prediction = response.json()['Predicted Sales']\n",
    "        st.success(f\"Predicted Sales: {prediction}\")\n",
    "    else:\n",
    "        st.error(\"Error making prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249",
   "metadata": {
    "id": "beq1RbMhUQmi"
   },
   "source": [
    "## Dependencies File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250",
   "metadata": {
    "id": "3BcfxQ1VGjb6",
    "outputId": "723642e5-8e68-4015-d2d4-2cf7f169f5b6"
   },
   "outputs": [],
   "source": [
    "%%writefile frontend_files/requirements.txt\n",
    "pandas==2.2.2\n",
    "numpy==2.0.2\n",
    "scikit-learn==1.6.1\n",
    "xgboost==2.1.4\n",
    "joblib==1.4.2\n",
    "Werkzeug==2.2.2\n",
    "flask==2.2.2\n",
    "gunicorn==20.1.0\n",
    "requests==2.28.1\n",
    "uvicorn[standard]\n",
    "streamlit==1.43.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251",
   "metadata": {
    "id": "B-zE77eWcuGo"
   },
   "source": [
    "## DockerFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252",
   "metadata": {
    "id": "Tl5MzECZGufV",
    "outputId": "e546f0d8-43c3-4474-8403-262992eb218f"
   },
   "outputs": [],
   "source": [
    "%%writefile frontend_files/Dockerfile\n",
    "# Use a minimal base image with Python 3.9 installed\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set the working directory inside the container to /app\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy all files from the current directory on the host to the container's /app directory\n",
    "COPY . .\n",
    "\n",
    "# Install Python dependencies listed in requirements.txt\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "# Define the command to run the Streamlit app on port 8501 and make it accessible externally\n",
    "CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\", \"--server.enableXsrfProtection=false\"]\n",
    "\n",
    "# NOTE: Disable XSRF protection for easier external access in order to make batch predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253",
   "metadata": {
    "id": "5Re8ovwv9Rb5"
   },
   "source": [
    "## Uploading Files to Hugging Face Space (Streamlit Space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254",
   "metadata": {
    "id": "GKFHV8c0qs-l",
    "outputId": "a6352a01-a9c2-40a1-bdf9-5f450eeaf806"
   },
   "outputs": [],
   "source": [
    "# for hugging face space authentication to upload files\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "repo_id = \"surnellas/SuperKart_Frontend\"  # Your Hugging Face space id\n",
    "\n",
    "# Initialize the API\n",
    "api = HfApi()\n",
    "\n",
    "# Upload Streamlit app files stored in the folder called deployment_files\n",
    "api.upload_folder(\n",
    "    folder_path=\"frontend_files\",  # Local folder path\n",
    "    repo_id=repo_id,  # Hugging face space id\n",
    "    repo_type=\"space\",  # Hugging face repo type \"space\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255",
   "metadata": {
    "id": "7b932048"
   },
   "source": [
    "### **Hugging face links**\n",
    "\n",
    "- Frontend:  https://huggingface.co/spaces/surnellas/SuperKart_Frontend\n",
    "- Backend:   https://huggingface.co/spaces/surnellas/SuperKart_Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256",
   "metadata": {
    "id": "2fc35b1a"
   },
   "source": [
    "## **Screenshot of the Backend**\n",
    "\n",
    "- Colab is not embedding screenshots properly in Markdown cell. SO I loaded this screenshots using OpenCV to embed the image in the Colab.\n",
    "- I captured this screenshot when Backend is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "id": "ChQqtLd1XTm6",
    "outputId": "6fc9f559-5b6d-478a-b807-49902c9534e3"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path to the image file\n",
    "image_path = '/content/Backend.png'\n",
    "\n",
    "# Load the image using OpenCV\n",
    "# cv2.imread reads an image in BGR format by default\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was loaded successfully\n",
    "if image is not None:\n",
    "    # Convert the image from BGR to RGB for displaying with Matplotlib\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image using Matplotlib\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title('Backend Hugging Face Screenshot')\n",
    "    plt.axis('off') # Hide axes\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Error: Could not load image from {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258",
   "metadata": {
    "id": "af07080c"
   },
   "source": [
    "<<<<<<< LOCAL CELL DELETED >>>>>>>\n",
    "![Screenshot 2025-08-13 204222-HF-Backend.png](<attachment:/content/Screenshot 2025-08-13 204222-HF-Backend.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259",
   "metadata": {
    "id": "50bbf934"
   },
   "source": [
    "## **Screenshot of the frontend**\n",
    "\n",
    "1.  This is the frontend screenshot it shows the input entered and the output text for the prediction.\n",
    "2. The REST JSON aPI is executed. The code is above with links.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "gUsoYjuRXk_8",
    "outputId": "168b9391-0eab-4f12-ff66-8255928cb7fd"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path to the image file\n",
    "image_path = '/content/Frontend.png'\n",
    "\n",
    "# Load the image using OpenCV\n",
    "# cv2.imread reads an image in BGR format by default\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was loaded successfully\n",
    "if image is not None:\n",
    "    # Convert the image from BGR to RGB for displaying with Matplotlib\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the image using Matplotlib\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title('Frontend Hugging Face screenshot')\n",
    "    plt.axis('off') # Hide axes\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Error: Could not load image from {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261",
   "metadata": {
    "id": "5edaf0e9"
   },
   "source": [
    "<<<<<<< LOCAL CELL DELETED >>>>>>>\n",
    "![Screenshot 2025-08-13 204412_Frontend.png](<attachment:/content/Screenshot 2025-08-13 204412_Frontend.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262",
   "metadata": {
    "id": "e4213339"
   },
   "source": [
    "# **Actionable Insights and Business Recommendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263",
   "metadata": {
    "id": "54464f04"
   },
   "source": [
    "## **Insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264",
   "metadata": {
    "id": "gzTwDHFNZMjo"
   },
   "source": [
    "- The dataset is well formed with no invalidate data. Minor correction in PSC (has some \"reg\" for \"Regular\") was easily corrected. No elaborate treatment was required for missing values or null values.\n",
    "- There are 4 stores in dataset - Two are Super Markets, one is Deparmental Store and Other is a Food Market.\n",
    "- The dataset contains Products and Stores attributes. The Products are 8763 in number and they do ot overlap across Stores.\n",
    "- The product and Stores have many attributes and thre correlation has been explored in a detailed manner to find correlations and make an accurate prediction with more 91% accuracy as validated by the Cross Validations scores.\n",
    "\n",
    "The insights are as follows:\n",
    "<<<<<<< local\n",
    "\n",
    "- The 4 stores in the dataset are having mutually exclusive products. They are meant for different market segementations. By design the store types are designed for addressing differnt needs of the customers.\n",
    "- The supermarkets Type2 addresses daily consumption products which are most commonly purchased by every household on a regualr basis. It targets lower quartile to Middle and Upper middle qaurtiles of the MRPS segements for the products.\n",
    "- Type1 supermarket products are an intelligent mix of Premium products and a Selected set upper Middle quartile of frequently used products. They have an MRP range above the Super Markets but lesser than Depratmental stores.\n",
    "=======\n",
    "- The 4 stores in the dataset are having mutually exclusive products. They are meant for different market segementations. By design the store types are designed for addressing differnt needs of the customers.\n",
    "- The supermarkets Type2 addresses daily used products that are most commonly purchased by every household on a regualr basis. It targets lower quartile to Middle and Upper middle qaurtiles of the MRPS segements for the products.\n",
    "- Type1 supermarkets are an intelligent mix of Near Premium products and Selected upper Middle quartile of frequently used products. They have an MRP range above the Super Markets but lesser than Depratmental stores. They address \"high\" are stores in the Tier 2 spaces of the cities.\n",
    ">>>>>>> remote\n",
    "- The Departmental Stores is more Premium Product Segement. It targets higher MRPs. Even if there are fewer number of orders the overall sales will be much higher than the supermarkets like Type1. They address Tier 1 cities.\n",
    "- The food Market is like convienence store for \"Small\" stores addressing very low MRPs for products.\n",
    "- The products are classified into 16 different types. Broadly there are Food items, Drinks and Non Consumables. Out of these items the Food items comprise a large portion of sales, the highest being Fruits and Vegetables for all the stores.\n",
    "- the key attributes for products are\n",
    "   - Weight: Product Weight\n",
    "        - It is observed that lower Middle, Middle and Upper Middle Quartiles the Weight of the product is highly correlated with the sales positiely. For lower Qaurtiles and Upper / Outlier quarties Weight is not a well correlated, meaning lower weight can have higher MRP and vice versa.\n",
    "        - In general Super market are focused in this MRP segements hence the correlation of Weights and sales is very high for Super Markets.\n",
    "   - MRP: the seeling price of the product\n",
    "        - By design it is qualifying the Store Type. The 4 Stores types are addressing differnt MRP segements.\n",
    "   - Area: display Area allocated by the product.\n",
    "        - totally controlled by Store adminstration to maximize sales. It succeded in more than 90% of the cases but there a few cases where Areas aalocated are -vely proportion to sales. The business have to take these on case by cases and adjust the allocation areas.\n",
    "        - The heatmap section of this report details the products and store combination where -ve correlations are observed.\n",
    "- The stores by design are addressing different MRPs. They monitor sales and take there own decisions on how to adjust the Display allocation Area for the products so that sales is maximized. The product Weights influence the sales as the Area and MRPs are correlated to the Weights.\n",
    "- The correlation of Product attributes with Sales is a very complex relationship it is difficult to visualize. The Heatmap and Pairplot provide ver weak insights. This is a perfect ase for an ensemble model to find out deeper realtionships and convert this into ML model.\n",
    "<<<<<<< local\n",
    "- Another Attribute of the product is Product Sugar content level. It is fairly straght forward showig high sales for all products which are \"Low Sugar\". The sales of \"Regular\" sugar content is relatively low compared to \"Low Sugar\". this attribute also classifies \"Non Consumable\" products as \"No Sugar\"\n",
    "- The store attributes are more straight forward\n",
    "   - Store Location tpye: It is indicative for which Tier the Store is going to Target. Tier 1 for high cost of living areas and Tier 3 for low cost of living. tier 2 is in between. In the dataset provided  Super Market type store are in \"Tier 2\", Departmental Stores is in \"Tier 1\" and Food MArket is in \"Tier 3\"\n",
    "   - Store Size classifies stores as \"High\", \"Medium\" and \"Small\". Thisis simply reflective of the area of store it is not anyway  reflective of Allocation Areas and/or MRPs. IN the Dataset provided \"High\" is a \"Tier 2\" Super Market Type 1. Medium is Departmental and Super Market Type2. Tier 3 Food Market is \"Small\"\n",
    "   - Store Establishment Year simply gives in which year the store was established. It is observed that the store established in 1987 had very good management of MRPs Vs Weight Vs Area and correlations of this Type 1 Vintage 1987 store was the highest with Sales. Other stores must adopt the best practices of allocation Areas and Choosing MRPs / Weight from this Store.\n",
    "   -overall one can say the sales prediction is complex combinations of product and Store attributes. The model has to consider all  all Product and Stores attributes. There are many hidden relationships between Store and Product attributes which are best enalyzed by ensemble models.\n",
    "\n",
    "\n",
    "=======\n",
    "- One Attribute of the product is Product Sugar content level. It is fairly straght forward showig high sales for all products which are \"Low Sugar\". The sales of \"Regular\" sugar content is relatively low compared to \"Low Sugar\". this attribute also classifies \"Non Consumable\" products as \"No Sugar\"\n",
    "- The store attributes are more straight forward\n",
    "   - Store Location tpye: It is indicative for which Tier the Store is going to Target. Tier 1 for high cost of living areas and Tier 3 for low cost of living. tier 2 is in between. In the dataset provided  Super Market type store are in \"Tier 2\", Departmental Stores is in \"Tier 1\" and Food MArket is in \"Tier 3\"\n",
    "   - Store Size classifies stores as \"High\", \"Medium\" and \"Small\". Thisis simply reflective of the area of store it is not anyway  reflective of Allocation Areas and/or MRPs. IN the Dataset provided \"High\" is a \"Tier 2\" Super Market Type 1. Medium is Departmental and Super Market Type2. Tier 3 Food Market is \"Small\"\n",
    "   - Store Establishment Year simply gives in which year the store was established. Itis observed that the store established in 1987 had very good management of MRPs Vs Weight Vs Area and correlations of this Type 1 Vintage 1987 store was the highest with Sales. Other stores must adopt the best practices of allocation Areas and Choosing MRPs / Weight from this Store.\n",
    "   -overall one an say the sales prediction is deep model of product and Store attributes. There cannot be a generic model for all Product ans Stores. There are many hidden relationships between Store and Product attributes which are best enalyzed by ensemble models.\n",
    ">>>>>>> remote\n",
    "   - the model built is doing a good job on this. It has 92% Accuracy and ready for production deployment.\n",
    "   - It has been deployed on Hugging Face with Steamlit for consumption of any demos.\n",
    "   - The links for Frontend and ABckend are listed in section prior to this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265",
   "metadata": {
    "id": "4ed27f1a"
   },
   "source": [
    "## **Recomendations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266",
   "metadata": {
    "id": "d19b1bc0"
   },
   "source": [
    "<<<<<<< local\n",
    "Display Areas must pripritze products with higher sales as opposed to products with low sales. \n",
    "-  Check analmolies listed in heatmap for -ve correlations for Product Area Allocations. \n",
    "- Use the data in heatmap to correct allocations such that Areas correspond to the sales correctly. \n",
    "\n",
    "Double down on best selling items:\n",
    "- Prioritize Fruits & Vegetables, Snack Foods, Dairy, Frozen for end-caps, promos, and in-stock rigor.\n",
    "\n",
    "=======\n",
    "Double down on best selling items:\n",
    "- Prioritize Fruits & Vegetables, Snack Foods, Dairy, Frozen for end-caps, promos, and in-stock rigor.\n",
    "\n",
    ">>>>>>> remote\n",
    "Store-type strategy\n",
    "- Supermarket Type2: Treat as growth engine—run cross-category bundles and high-velocity replenishment.\n",
    "- Departmental/Type1: Use targeted promos to lift basket size; mirror bestsellers from Type2.\n",
    "- Food Mart: Curate a tight, high-turn core; avoid long-tail inventory.\n",
    "\n",
    "Size & layout\n",
    "- High/Medium stores: Keep space-rich for top categories; protect space against fragmentation.\n",
    "- Small stores: Focus on premium, faster-moving items (higher MRP bins) to lift average ticket.\n",
    "\n",
    "Pricing & portfolio:\n",
    "- Create good-better-best ladders in categories where MRP bins show step-ups in average sales.\n",
    "- Use price anchoring: display premium SKUs to increase perceived value and trade-up.\n",
    "\n",
    "Assortment by city tier & sugar\n",
    "- In health-conscious catchments (if you can map city tiers to affluence), increase low/regular sugar variants in Dairy/Snacks and message “better-for-you” options.\n",
    "\n",
    "Promo & ops\n",
    "- Run basket-building offers that pair produce with dairy/snacks (complements with highest sales).\n",
    "- Track out-of-stock on top 10 products and enforce service-level SLAs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5fd3cabe",
    "YyzOQ8pBY93N"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
